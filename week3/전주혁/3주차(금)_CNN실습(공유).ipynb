{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3주차(금)_CNN실습(공유).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yunjeong-chang/2022_AI_Practical_Course/blob/main/3%EC%A3%BC%EC%B0%A8(%EA%B8%88)_CNN%EC%8B%A4%EC%8A%B5(%EA%B3%B5%EC%9C%A0).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. 데이터 불러오기\n",
        "https://drive.google.com/file/d/1M8KwdmGm8EWCn_IEWAcctbUJBww-M3cF/view?usp=sharing\n",
        "\n",
        "1. 위 링크에 있는 zip 파일을 '드라이브에 바로가기 추가'하기(안되면 그냥 다운로드 후 내 드라이브에 업로드)\n",
        "2. GPU 설정 후, 드라이브 마운트\n",
        "3. zip 파일 풀기 (약 2분 소요)"
      ],
      "metadata": {
        "id": "TDekbT7bHvKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGhvgArkTt-m",
        "outputId": "e159ba6d-2b9d-41bc-a740-633cbf059fd4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip -uq “압축을 풀 zip 파일의 경로” -d “압축을 풀고자 하는 폴더의 경로”\n",
        "!unzip -uq /content/drive/MyDrive/plant-leaf-dataset.zip -d /content/drive/MyDrive/plant-leaf-dataset"
      ],
      "metadata": {
        "id": "0CrELDhBI3yO"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAmOFLpdtXV5"
      },
      "source": [
        "### 1. 데이터 분할을 위한 디렉토리 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH7lRtSlpG7c"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "original_dataset_dir = '/content/drive/MyDrive/plant-leaf-dataset/plant-leaf-dataset' #데이터셋이 위치한 경로 지정  \n",
        "classes_list = os.listdir(original_dataset_dir) #해당 경로 하위에 있는 모든 폴더의 목록을 가져옴(폴더 목록 == 클래스 목록)\n",
        " \n",
        "base_dir = '/content/drive/MyDrive/plant-leaf-dataset/plant-leaf-new-dataset' #train/val/test로 분할한 데이터를 저장할 폴더 생성\n",
        "os.mkdir(base_dir)\n",
        " \n",
        "train_dir = os.path.join(base_dir, 'train') #train 폴더 생성\n",
        "os.mkdir(train_dir)\n",
        "validation_dir = os.path.join(base_dir, 'val') #\bvalidation 폴더 생성\n",
        "os.mkdir(validation_dir)\n",
        "test_dir = os.path.join(base_dir, 'test') #test 폴더 생성\n",
        "os.mkdir(test_dir)\n",
        "\n",
        "for cls in classes_list: #train/val/test 폴더에 각각 클래스 목록 폴더를 생성    \n",
        "    os.mkdir(os.path.join(train_dir, cls))\n",
        "    os.mkdir(os.path.join(validation_dir, cls))\n",
        "    os.mkdir(os.path.join(test_dir, cls))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. train/validation/test 데이터 분할 및 클래스 별 데이터 수 확인"
      ],
      "metadata": {
        "id": "eKJ1QY2e28i4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_v0a0PUSrdnZ",
        "outputId": "ac0bc6d1-83aa-4197-c7e5-dbe490b35a0c"
      },
      "source": [
        "\n",
        "import math\n",
        "for cls in classes_list: #모든 클래스에 대한 작업 반복\n",
        "    path = os.path.join(original_dataset_dir, cls) \n",
        "    fnames = os.listdir(path) #path 위치에 존재하는 모든 이미지 파일의 목록을 fnames에 저장\n",
        "    \n",
        "    #train/validation/test 의 비율을 6:2:2로 (데이터 규모에 따라 조정 가능)\n",
        "    train_size = math.floor(len(fnames) * 0.6)\n",
        "    validation_size = math.floor(len(fnames) * 0.2)\n",
        "    test_size = math.floor(len(fnames) * 0.2)\n",
        "    \n",
        "    #train\n",
        "    train_fnames = fnames[:train_size] #train 데이터에 해당하는 파일의 이름을 train_fnames에 저장\n",
        "    for fname in train_fnames: #train 데이터에 대해 for문의 내용 반복\n",
        "        src = os.path.join(path, fname) #복사할 원본 파일의 경로 지정\n",
        "        dst = os.path.join(os.path.join(train_dir, cls), fname) #복사한 후 저장할 파일의 경로 지정\n",
        "        shutil.copyfile(src, dst) #src의 경로에 해당하는 파일을 dst의 경로에 지정\n",
        "    \n",
        "    #validation\n",
        "    validation_fnames = fnames[train_size:(validation_size + train_size)]\n",
        "    for fname in validation_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(validation_dir, cls), fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "        \n",
        "    #test    \n",
        "    test_fnames = fnames[(train_size + validation_size):(test_size + validation_size + train_size)]\n",
        "    for fname in test_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(test_dir, cls), fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "\n",
        "    print(\"class(\",cls,\") Train:\",len(train_fnames), \"Validation:\",len(validation_fnames), \"Test:\",len(test_fnames))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class( Apple___healthy ) Train: 987 Validation: 329 Test: 329\n",
            "class( Grape___healthy ) Train: 253 Validation: 84 Test: 84\n",
            "class( Grape___Black_rot ) Train: 708 Validation: 236 Test: 236\n",
            "class( Peach___Bacterial_spot ) Train: 1378 Validation: 459 Test: 459\n",
            "class( Potato___healthy ) Train: 91 Validation: 30 Test: 30\n",
            "class( Potato___Early_blight ) Train: 600 Validation: 200 Test: 200\n",
            "class( Corn___Common_rust ) Train: 715 Validation: 238 Test: 238\n",
            "class( Strawberry___Leaf_scorch ) Train: 671 Validation: 223 Test: 223\n",
            "class( Apple___Apple_scab ) Train: 378 Validation: 126 Test: 126\n",
            "class( Strawberry___healthy ) Train: 273 Validation: 91 Test: 91\n",
            "class( Peach___healthy ) Train: 216 Validation: 72 Test: 72\n",
            "class( Corn___healthy ) Train: 697 Validation: 232 Test: 232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYCY0sqFso7L"
      },
      "source": [
        "### 3. 기본 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucURIVBmsnmC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "202d1108-72a3-4bca-ec2f-d7957b4b122a"
      },
      "source": [
        "import torch\n",
        "import os\n",
        " \n",
        "USE_CUDA = torch.cuda.is_available() #GPU 사용 가능한지 확인하는 메서드(사용할 수 있으면 TRUE, 없으면 FALSE 반환)\n",
        "print(USE_CUDA)\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\") #DEVICE 변수에 TRUE 이면 cuda를 FALSE 이면 cpu를 저장\n",
        "print(DEVICE)\n",
        "\n",
        "BATCH_SIZE = 512 #배치사이즈 지정\n",
        "EPOCH = 7 #에포크 지정\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "data_transforms = { # transforms.Compose()는 이미지 전처리, Augmentation 등 사용, Augmentation이란? 좌우 반전, 밝기 조절, 이미지 확대 등 노이즈를 주어 더 강한 모델을 만들어 주는 기법\n",
        "    'train': transforms.Compose([transforms.Resize([64,64]), # Resize -> 이미지의 크기를 64x64로 조정                    \n",
        "                                 transforms.RandomHorizontalFlip(), #RandomHorizontalFlip -> 이미지를 무작위로 좌우 반전\n",
        "                                 transforms.RandomVerticalFlip(), #RandomVerticalFlip -> 이미지를 무작위로 상하 반전\n",
        "                                 transforms.RandomCrop(52), #RandomCrop -> 이미지의 일부를 랜덤하게 잘라서 52x52 사이즈로 변경\n",
        "                                 transforms.ToTensor(), # ToTensor -> 이미지를 텐서 형태로 변환하고, 모든 값을 0~1 사이로 변경\n",
        "                                 transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), #Normalize ->정규화를 위해선 평균값과 표준편차값이 필요\n",
        "                                                                                                        #            첫번째[]는 R,G,B 채널 값에서 정규화를 적용할 평균값 \n",
        "                                                                                                        #            두번째[]는 R,G,B 채널 값에서 정규화를 적용할 표준편차값 \n",
        "                                                                                                        #            이 값은 이미지넷 데이터의 값이고, 정규화는 Local Minimum에 빠지는 것을 방지\n",
        "    'val': transforms.Compose([transforms.Resize([64,64]), \n",
        "                               #validation data는 Augmentation에 해당하는 부분을 제외하고 동일하게 전처리 \n",
        "                               transforms.RandomCrop(52), \n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ])\n",
        "}"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 데이터 로더"
      ],
      "metadata": {
        "id": "e0zmtPpS9oAW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STs5oRi2sy12",
        "outputId": "96bc47f2-e8ff-4ea0-fec1-da4cd02b547f"
      },
      "source": [
        "from torchvision.datasets import ImageFolder #이미지 데이터는 하나의 클래스가 하나의 폴더에 대응되기 때문에 데이터셋을 불러올 때 ImageFolder를 사용\n",
        "\n",
        "# ImageFolder로 데이터셋 불러오기 -> root : 데이터 불러 올 경로 설정, transform : 앞서 설정한 전처리 방법 지정(불러오기 편하게 딕셔너리 형태로 구성)\n",
        "image_datasets = {x: ImageFolder(root=os.path.join(base_dir, x), transform=data_transforms[x]) for x in ['train', 'val']} \n",
        "\n",
        "# DataLoader로 불러온 이미지 데이터를 주어진 조건에 따라 미니 배치 단위로 분리 -> shuffle=True : 데이터의 순서가 섞여 학습시에 Label 정보의 순서를 기억하는 것을 방지 할 수 있음 필수!\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE, shuffle=True, num_workers=4) for x in ['train', 'val']} \n",
        "\n",
        "#train/validation의 총 개수를 저장\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "#12개 클래스의 목록을 저장\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "print(class_names)\n",
        "print(len(class_names))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Apple___Apple_scab', 'Apple___healthy', 'Corn___Common_rust', 'Corn___healthy', 'Grape___Black_rot', 'Grape___healthy', 'Peach___Bacterial_spot', 'Peach___healthy', 'Potato___Early_blight', 'Potato___healthy', 'Strawberry___Leaf_scorch', 'Strawberry___healthy']\n",
            "12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. 전이학습 모델 불러오기\n",
        "1. 모델만 불러와서 구조 print 해보기\n",
        "2. 분류층 바꾸고 print 해보기"
      ],
      "metadata": {
        "id": "Uy5j3kc79q6x"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZEFZgmTs2Vt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdf036ac-e359-4a6b-9106-ba913b32b938"
      },
      "source": [
        "from torchvision import models #pytorch 공식문서에서 확인 한 것처럼, 여기서 여러 모델을 불러올 수 있음\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "#resnet18/34/50 \n",
        "model = models.resnet50(pretrained=True) #pretrained=True로 설정하면 pre-trained model의 parameter값을 그대로 가져옴, False로 설정하면 모델의 아키텍처만 가져오고 parameter는 랜덤 설정\n",
        "num_ftrs = model.fc.in_features #모델의 마지막 레이어의 입력 채널의 수를 저장(in_features는 해당 레이어의 입력 채널 수를 의미)   \n",
        "model.fc = nn.Linear(num_ftrs, len(class_names)) #모델의 마지막 레이어를 새로운 레이어로 교체 (입력 채널 수는 기존 레이어와 동일, 출력 채널 수를 우리가 원하는 수로 설정하는 것! 여기서는 클래스 수 12개) \n",
        "\n",
        "'''\n",
        "#vgg16/19\n",
        "model = models.vgg16(pretrained=True)\n",
        "#model.classifier[6].out_features = len(class_names) #마지막 레이어를 교체하는 방법이 약간 다름, print 해서 구조 확인하면서 이해\n",
        "\n",
        "#mobilenet_v2\n",
        "model = models.mobilenet_v2(pretrained=True)\n",
        "#model.classifier[1].out_features = len(class_names)\n",
        "\n",
        "#mobilnet_v3_small\n",
        "model = models.mobilenet_v3_small(pretrained=True)\n",
        "#model.classifier[3].out_features = len(class_names)\n",
        "'''\n",
        "\n",
        "model = model.to(DEVICE) #모델 gpu에 태우기\n",
        "print(model)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=2048, out_features=12, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Layer Freeze"
      ],
      "metadata": {
        "id": "4zzyFflRf13T"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wf8IIPgs3vs"
      },
      "source": [
        "cnt = 0 #몇 번째 Layer인지 나타내는 변수 cnt 설정\n",
        "for child in model.children(): #모델의 모든 Layer 정보를 담고 있음 (vgg, mobilenet 계열은 model.features)\n",
        "    cnt += 1 \n",
        "    if cnt < 5: #resnet50기준 10개의 Layer중 1~5개는 Freeze하고, 6~10은 학습 시 parameter를 업데이트 하도록!\n",
        "        #print(child)\n",
        "        for param in child.parameters(): #vgg, mobilenet 계열은 model.features.parameters()\n",
        "            param.requires_grad = False  #False -> NO UPDATE(FREEZE), True -> UPDATE(기본값)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. 손실함수, 최적화함수, 스케쥴러 설정\n",
        "- Adam vs SGD\n",
        "- learning rate는 작게!\n",
        "- 미리 학습 코드까지 실행!"
      ],
      "metadata": {
        "id": "onKCFqbZf9oZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#모델 학습에 사용하는 Loss 함수를 지정\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#Optimizer는 Adam, filter와 lambda를 사용하는 이유 : param.requires_grad = True로 설정된 Layer의 parameter만을 업데이트 하기 위해서!\n",
        "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.00001) \n",
        " \n",
        "from torch.optim import lr_scheduler\n",
        "# 에포크에 따라 Learning Rate를 변경하는 역할 (7 에포크마다 0.1씩 곱해 LR을 감소시킴), Why? : 학습 보폭을 정하는 일은 매우 중요한데, 처음엔 크게 -> 학습 진행될 수록 작게 설정하는 것이 좋다고 알려짐, but 아직 연구중\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "metadata": {
        "id": "LfwDUXcaD_uD"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. 모델 학습 및 저장"
      ],
      "metadata": {
        "id": "t86IqtKnK8Qr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXFjVMs3s5Jv"
      },
      "source": [
        "# 전이학습 모델 학습 및 검증\n",
        "import time\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n",
        "    \n",
        "    train_losses , train_accuracy = [],[] #그래프 그리기 위해서 train에 대한 loss,accuracy 저장\n",
        "    val_losses , val_accuracy = [],[] #그래프 그리기 위해서 validation에 대한 loss,accuracy 저장\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())  #정확도가 가장 높은 모델을 저장\n",
        "    best_acc = 0.0 #정확도가 가장 높은 모델의 정확도 저장\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print('-------------- epoch {} ----------------'.format(epoch+1)) \n",
        "        since = time.time() #한 에포크 돌 때 소요되는 시간 측정(시작 시각 저장)                                    \n",
        "        for phase in ['train', 'val']: #한 에포크 돌 때 train 한 번, validation 한 번씩 각각 진행\n",
        "            if phase == 'train': \n",
        "                model.train() #train이면 학습 모드\n",
        "            else:\n",
        "                model.eval() #validation이면 평가 모드(평가 때 사용하지 말아야 할 작업들 알아서 꺼줌, dropout이나 batchnorm layer 같은 것들)     \n",
        " \n",
        "            running_loss = 0.0   #모든 데이터의 loss를 합해서 저장\n",
        "            running_corrects = 0 #정확하게 예측한 경우의 수를 저장\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]: #모델의 현재 모드(train or validation)에 해당하는 Dataloader에서 데이터를 받는 for문\n",
        "                inputs = inputs.to(DEVICE) #데이터를 gpu에 태움 \n",
        "                labels = labels.to(DEVICE) #데이터의 라벨값을 gpu에 태움\n",
        "                \n",
        "                optimizer.zero_grad() #학습 진행하면 이전 Batch의 Gradient값이 Optimizer에 저장될 것이므로 초기화 해주고 시작해야 함\n",
        "                \n",
        "                with torch.set_grad_enabled(phase == 'train'): #set_grad_enabled를 이용하면 train 모드에서만 모델의 Gradient를 업데이트 하도록 설정 할 수 있음\n",
        "                    outputs = model(inputs) #드디어 데이터를 모델에 입력!\n",
        "                    _, preds = torch.max(outputs, 1) #모델에 입력된 데이터가 12개의 클래스에 속할 확률값 출력, 이 중 가장 높은 값의 인덱스를 예측값으로 preds에 저장\n",
        "                    loss = criterion(outputs, labels) #모델의 예측값과 정답값 사이의 Loss를 계산(criterion 함수는 위에서 미리 설정해 둔 것)\n",
        "    \n",
        "                    if phase == 'train':   \n",
        "                        loss.backward() #계산한 loss값을 이용하여 BackPropagation을 통해 계산한 Gradient값을 parameter에 할당하고,\n",
        "                        optimizer.step() #모델의 parameter 업데이트\n",
        " \n",
        "                running_loss += loss.item() * inputs.size(0) #모든 데이터의 loss를 합해서 저장하기 위해, 하나의 미니 배치에 대한 loss값에 데이터의 수를 곱해서 더함 (inputs.size(0)이 미니 배치의 수) \n",
        "                running_corrects += torch.sum(preds == labels.data) #예측값과 정답값이 같으면 증가!\n",
        "\n",
        "            if phase == 'train':  \n",
        "                scheduler.step() #위에서 미리 설정한 Scheduler 실행\n",
        " \n",
        "            epoch_loss = running_loss/dataset_sizes[phase] #해당 에포크의 loss를 계산하기 위해 running_loss를 데이터셋 사이즈로 나눔\n",
        "            epoch_acc = running_corrects.double()/dataset_sizes[phase] #정확도도 마찬가지로 running_corrects를 데이터셋 사이즈로 나눔\n",
        " \n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc)) #해당 에포크의 loss와 정확도를 매번 출력\n",
        "\n",
        "            if phase == 'train': #그래프 그리기 위해 train 데이터의 loss와 accuracy 따로 저장\n",
        "                train_losses.append(epoch_loss)\n",
        "                train_accuracy.append(epoch_acc)\n",
        "            if phase == 'val': #그래프 그리기 위해 \bvalidation 데이터의 loss와 accuracy 따로 저장\n",
        "                val_losses.append(epoch_loss)\n",
        "                val_accuracy.append(epoch_acc)\n",
        "          \n",
        "            if phase == 'val' and epoch_acc > best_acc: #validation 모드에서 정확도가 최고 정확도 보다 높으면 업데이트\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict()) #최고 정확도를 가진 모델을 best_model_wts 변수에 저장\n",
        " \n",
        "        time_elapsed = time.time() - since #한 에포크 돌 때 소요되는 시간 측정(종료 시각 - 시작 시각) \n",
        "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) #계산한 시간 분과 초로 출력\n",
        "\n",
        "    #학습 종료 후 \n",
        "    print('Best validation Acc: {:4f}'.format(best_acc)) #validation 중 최고 정확도 출력\n",
        "\n",
        "    #train과 validation의 loss, accuracy 그래프 출력 -> 과적합 여부 등 판단\n",
        "    plt.plot(range(1,len(train_losses)+1),train_losses,'bo',label = 'training loss')\n",
        "    plt.plot(range(1,len(val_losses)+1),val_losses,'r',label = 'validation loss')\n",
        "    plt.legend()\n",
        "    plt.plot(range(1,len(train_accuracy)+1),train_accuracy,'co',label = 'training accuracy')\n",
        "    plt.plot(range(1,len(val_accuracy)+1),val_accuracy,'m',label = 'validation accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    #정확도가 가장 높았던 모델을 불러와서 반환\n",
        "    model.load_state_dict(best_model_wts) \n",
        "    return model"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 848
        },
        "id": "EQ6wBtMAs6pw",
        "outputId": "caa5968f-3ff5-4229-d3e2-8999b7500f43"
      },
      "source": [
        "# 전이학습 실행\n",
        "model = train_model(model, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=EPOCH) \n",
        "\n",
        "# 반환 받은 정확도가 가장 높았던 모델을 torch.save 이용해서 저장 (모델 별로 이름 변경해서 저장!)\n",
        "torch.save(model, '/content/drive/MyDrive/plant-leaf-dataset/resnet50.pt')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------- epoch 1 ----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 2.2728 Acc: 0.2522\n",
            "val Loss: 2.0055 Acc: 0.4168\n",
            "Completed in 0m 33s\n",
            "-------------- epoch 2 ----------------\n",
            "train Loss: 1.8185 Acc: 0.5581\n",
            "val Loss: 1.5614 Acc: 0.6815\n",
            "Completed in 0m 32s\n",
            "-------------- epoch 3 ----------------\n",
            "train Loss: 1.4234 Acc: 0.7457\n",
            "val Loss: 1.2091 Acc: 0.7931\n",
            "Completed in 0m 31s\n",
            "-------------- epoch 4 ----------------\n",
            "train Loss: 1.0898 Acc: 0.8065\n",
            "val Loss: 0.9256 Acc: 0.8250\n",
            "Completed in 0m 32s\n",
            "-------------- epoch 5 ----------------\n",
            "train Loss: 0.8226 Acc: 0.8368\n",
            "val Loss: 0.7047 Acc: 0.8517\n",
            "Completed in 0m 31s\n",
            "-------------- epoch 6 ----------------\n",
            "train Loss: 0.6281 Acc: 0.8598\n",
            "val Loss: 0.5467 Acc: 0.8772\n",
            "Completed in 0m 32s\n",
            "-------------- epoch 7 ----------------\n",
            "train Loss: 0.4939 Acc: 0.8845\n",
            "val Loss: 0.4333 Acc: 0.9013\n",
            "Completed in 0m 32s\n",
            "Best validation Acc: 0.901293\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjkyMbUEuMqi"
      },
      "source": [
        "### 7. 모델 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPwYDjEHtCXm",
        "outputId": "f0f64c4e-d3d5-4938-a01a-c97119057065"
      },
      "source": [
        "# 전이학습 평가 전처리 (위에서 설명한 것과 동일)\n",
        "data_transforms = transforms.Compose([ \n",
        "        transforms.Resize([64,64]),  \n",
        "        transforms.RandomCrop(52),  \n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
        "        ])\n",
        "\n",
        "#경로 맞춰서 변경해 주세요!\n",
        "test_dataset = ImageFolder(root='/content/drive/MyDrive/plant-leaf-dataset/plant-leaf-new-dataset/test', transform=data_transforms) \n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "# 모델 평가 함수\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval() #모델을 평가 모드로 설정\n",
        "    test_loss = 0 #미니 배치 별로 loss를 합산해서 저장\n",
        "    correct = 0 #정확하게 예측한 수 저장   \n",
        "    with torch.no_grad(): #해당 메서드를 이용해서 parameter 업데이트 방지\n",
        "        for data, target in test_loader:  \n",
        "            data, target = data.to(DEVICE), target.to(DEVICE) #데이터와 라벨을 불러오면서 gpu에 태움  \n",
        "            output = model(data) #데이터를 모델에 입력           \n",
        "            test_loss += torch.nn.functional.cross_entropy(output,target, reduction='sum').item() #모델의 예측값과 정답값 사이의 loss 계산\n",
        "            pred = output.max(1, keepdim=True)[1]  #모델에 입력된 데이터가 12개의 클래스에 속할 확률값 출력, 이 중 가장 높은 값의 인덱스를 예측값으로 pred에 저장\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item() #target.view_as(pred)를 이용해 target의 텐서 구조를 pred의 텐서와 같은 모양으로 재정렬 (모델 만들 때 쓰는 view와 비슷 view는 숫자 직접 지정)\n",
        "                                                                  #eq는 비교 연산자로 pred와 target.view_as(pred)의 값이 일치하면 1, 일치하지 않으면 0 반환\n",
        "   \n",
        "    test_loss /= len(test_loader.dataset) #모든 미니 배치에서 합한 loss값을 배치 수로 나누어 loss값의 평균 구함\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset) #마찬가지로 정확도의 평균도 구함\n",
        "    \n",
        "    return test_loss, test_accuracy #계산한 Test 데이터의 loss와 정확도 반환\n",
        "\n",
        "# 전이학습 모델 평가 결과\n",
        "model=torch.load('/content/drive/MyDrive/plant-leaf-dataset/resnet50.pt') #torch.load를 이용해서 원하는 모델 불러오기!\n",
        "test_loss, test_accuracy = evaluate(model, test_loader) #평가 함수 이용해서 Test 데이터에 대한 loss 및 정확도 측정\n",
        "print('model test acc:  ', test_accuracy) #평가 정확도 출력"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model test acc:   90.04310344827586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(과제) 한 가지 이상의 변화를 준 후 학습을 돌려서 결과와 함께 간단한 설명을 업로드 해주세요 😀\n",
        "\n",
        "예시 : 다른 전이학습 모델 사용, freeze 시키는 구간 변화, 직접 짠 모델과의 성능 비교, 데이터 수의 변화, optimizer에 대한 실험, epoch 늘리기, 등등"
      ],
      "metadata": {
        "id": "maEk9ITatoai"
      }
    }
  ]
}
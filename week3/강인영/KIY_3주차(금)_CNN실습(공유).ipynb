{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KIY-3주차(금)_CNN실습(공유).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 0. 데이터 불러오기\n",
        "https://drive.google.com/file/d/1M8KwdmGm8EWCn_IEWAcctbUJBww-M3cF/view?usp=sharing\n",
        "\n",
        "1. 위 링크에 있는 zip 파일을 '드라이브에 바로가기 추가'하기(안되면 그냥 다운로드 후 내 드라이브에 업로드)\n",
        "2. GPU 설정 후, 드라이브 마운트\n",
        "3. zip 파일 풀기 (약 2분 소요)"
      ],
      "metadata": {
        "id": "TDekbT7bHvKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip -uq “압축을 풀 zip 파일의 경로” -d “압축을 풀고자 하는 폴더의 경로”\n",
        "!unzip -uq /content/drive/MyDrive/plant-leaf-dataset.zip -d /content/drive/MyDrive/plant-leaf-dataset"
      ],
      "metadata": {
        "id": "0CrELDhBI3yO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAmOFLpdtXV5"
      },
      "source": [
        "### 1. 데이터 분할을 위한 디렉토리 생성"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUuZGNowRmHD",
        "outputId": "64fe0a2c-141a-4afc-945d-17e9e577d6a4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH7lRtSlpG7c"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "original_dataset_dir = '/content/drive/MyDrive/plant-leaf-dataset/plant-leaf-dataset' #데이터셋이 위치한 경로 지정  \n",
        "classes_list = os.listdir(original_dataset_dir) #해당 경로 하위에 있는 모든 폴더의 목록을 가져옴(폴더 목록 == 클래스 목록)\n",
        " \n",
        "base_dir = '/content/drive/MyDrive/plant-leaf-dataset/plant-leaf-new-dataset' #train/val/test로 분할한 데이터를 저장할 폴더 생성\n",
        "os.mkdir(base_dir)\n",
        " \n",
        "train_dir = os.path.join(base_dir, 'train') #train 폴더 생성\n",
        "os.mkdir(train_dir)\n",
        "validation_dir = os.path.join(base_dir, 'val') #\bvalidation 폴더 생성\n",
        "os.mkdir(validation_dir)\n",
        "test_dir = os.path.join(base_dir, 'test') #test 폴더 생성\n",
        "os.mkdir(test_dir)\n",
        "\n",
        "for cls in classes_list: #train/val/test 폴더에 각각 클래스 목록 폴더를 생성    \n",
        "    os.mkdir(os.path.join(train_dir, cls))\n",
        "    os.mkdir(os.path.join(validation_dir, cls))\n",
        "    os.mkdir(os.path.join(test_dir, cls))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. train/validation/test 데이터 분할 및 클래스 별 데이터 수 확인"
      ],
      "metadata": {
        "id": "eKJ1QY2e28i4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_v0a0PUSrdnZ",
        "outputId": "3c979557-20a7-40eb-bd46-6826c6d42420"
      },
      "source": [
        "import math\n",
        "for cls in classes_list: #모든 클래스에 대한 작업 반복\n",
        "    path = os.path.join(original_dataset_dir, cls) \n",
        "    fnames = os.listdir(path) #path 위치에 존재하는 모든 이미지 파일의 목록을 fnames에 저장\n",
        "    \n",
        "    #train/validation/test 의 비율을 6:2:2로 (데이터 규모에 따라 조정 가능)\n",
        "    train_size = math.floor(len(fnames) * 0.6)\n",
        "    validation_size = math.floor(len(fnames) * 0.2)\n",
        "    test_size = math.floor(len(fnames) * 0.2)\n",
        "    \n",
        "    #train\n",
        "    train_fnames = fnames[:train_size] #train 데이터에 해당하는 파일의 이름을 train_fnames에 저장\n",
        "    for fname in train_fnames: #train 데이터에 대해 for문의 내용 반복\n",
        "        src = os.path.join(path, fname) #복사할 원본 파일의 경로 지정\n",
        "        dst = os.path.join(os.path.join(train_dir, cls), fname) #복사한 후 저장할 파일의 경로 지정\n",
        "        shutil.copyfile(src, dst) #src의 경로에 해당하는 파일을 dst의 경로에 지정\n",
        "    \n",
        "    #validation\n",
        "    validation_fnames = fnames[train_size:(validation_size + train_size)]\n",
        "    for fname in validation_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(validation_dir, cls), fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "        \n",
        "    #test    \n",
        "    test_fnames = fnames[(train_size + validation_size):(test_size + validation_size + train_size)]\n",
        "    for fname in test_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(test_dir, cls), fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "\n",
        "    print(\"class(\",cls,\") Train:\",len(train_fnames), \"Validation:\",len(validation_fnames), \"Test:\",len(test_fnames))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class( Apple___healthy ) Train: 987 Validation: 329 Test: 329\n",
            "class( Grape___healthy ) Train: 253 Validation: 84 Test: 84\n",
            "class( Grape___Black_rot ) Train: 708 Validation: 236 Test: 236\n",
            "class( Peach___Bacterial_spot ) Train: 1378 Validation: 459 Test: 459\n",
            "class( Potato___healthy ) Train: 91 Validation: 30 Test: 30\n",
            "class( Potato___Early_blight ) Train: 600 Validation: 200 Test: 200\n",
            "class( Corn___Common_rust ) Train: 715 Validation: 238 Test: 238\n",
            "class( Strawberry___Leaf_scorch ) Train: 671 Validation: 223 Test: 223\n",
            "class( Apple___Apple_scab ) Train: 378 Validation: 126 Test: 126\n",
            "class( Strawberry___healthy ) Train: 273 Validation: 91 Test: 91\n",
            "class( Peach___healthy ) Train: 216 Validation: 72 Test: 72\n",
            "class( Corn___healthy ) Train: 697 Validation: 232 Test: 232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYCY0sqFso7L"
      },
      "source": [
        "### 3. 기본 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucURIVBmsnmC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0715c80-4c92-42d4-9f03-17fc36d3bdc0"
      },
      "source": [
        "import torch\n",
        "import os\n",
        " \n",
        "USE_CUDA = torch.cuda.is_available() #GPU 사용 가능한지 확인하는 메서드(사용할 수 있으면 TRUE, 없으면 FALSE 반환)\n",
        "print(USE_CUDA)\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\") #DEVICE 변수에 TRUE 이면 cuda를 FALSE 이면 cpu를 저장\n",
        "print(DEVICE)\n",
        "\n",
        "BATCH_SIZE = 512 #배치사이즈 지정\n",
        "EPOCH = 5 #에포크 지정\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "data_transforms = { # transforms.Compose()는 이미지 전처리, Augmentation 등 사용, Augmentation이란? 좌우 반전, 밝기 조절, 이미지 확대 등 노이즈를 주어 더 강한 모델을 만들어 주는 기법\n",
        "    'train': transforms.Compose([transforms.Resize([64,64]), # Resize -> 이미지의 크기를 64x64로 조정                    \n",
        "                                 transforms.RandomHorizontalFlip(), #RandomHorizontalFlip -> 이미지를 무작위로 좌우 반전\n",
        "                                 transforms.RandomVerticalFlip(), #RandomVerticalFlip -> 이미지를 무작위로 상하 반전\n",
        "                                 transforms.RandomCrop(52), #RandomCrop -> 이미지의 일부를 랜덤하게 잘라서 52x52 사이즈로 변경\n",
        "                                 transforms.ToTensor(), # ToTensor -> 이미지를 텐서 형태로 변환하고, 모든 값을 0~1 사이로 변경\n",
        "                                 transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), #Normalize ->정규화를 위해선 평균값과 표준편차값이 필요\n",
        "                                                                                                        #            첫번째[]는 R,G,B 채널 값에서 정규화를 적용할 평균값 \n",
        "                                                                                                        #            두번째[]는 R,G,B 채널 값에서 정규화를 적용할 표준편차값 \n",
        "                                                                                                        #            이 값은 이미지넷 데이터의 값이고, 정규화는 Local Minimum에 빠지는 것을 방지\n",
        "    'val': transforms.Compose([transforms.Resize([64,64]), \n",
        "                               #validation data는 Augmentation에 해당하는 부분을 제외하고 동일하게 전처리 \n",
        "                               transforms.RandomCrop(52), \n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ])\n",
        "}"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 데이터 로더"
      ],
      "metadata": {
        "id": "e0zmtPpS9oAW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STs5oRi2sy12",
        "outputId": "e20566c1-f113-49a3-9200-19286baf3924"
      },
      "source": [
        "from torchvision.datasets import ImageFolder #이미지 데이터는 하나의 클래스가 하나의 폴더에 대응되기 때문에 데이터셋을 불러올 때 ImageFolder를 사용\n",
        "\n",
        "# ImageFolder로 데이터셋 불러오기 -> root : 데이터 불러 올 경로 설정, transform : 앞서 설정한 전처리 방법 지정(불러오기 편하게 딕셔너리 형태로 구성)\n",
        "image_datasets = {x: ImageFolder(root=os.path.join(base_dir, x), transform=data_transforms[x]) for x in ['train', 'val']} \n",
        "\n",
        "# DataLoader로 불러온 이미지 데이터를 주어진 조건에 따라 미니 배치 단위로 분리 -> shuffle=True : 데이터의 순서가 섞여 학습시에 Label 정보의 순서를 기억하는 것을 방지 할 수 있음 필수!\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE, shuffle=True, num_workers=4) for x in ['train', 'val']} \n",
        "\n",
        "#train/validation의 총 개수를 저장\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "#12개 클래스의 목록을 저장\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "print(class_names)\n",
        "print(len(class_names))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Apple___Apple_scab', 'Apple___healthy', 'Corn___Common_rust', 'Corn___healthy', 'Grape___Black_rot', 'Grape___healthy', 'Peach___Bacterial_spot', 'Peach___healthy', 'Potato___Early_blight', 'Potato___healthy', 'Strawberry___Leaf_scorch', 'Strawberry___healthy']\n",
            "12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. 전이학습 모델 불러오기\n",
        "1. 모델만 불러와서 구조 print 해보기\n",
        "2. 분류층 바꾸고 print 해보기"
      ],
      "metadata": {
        "id": "Uy5j3kc79q6x"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZEFZgmTs2Vt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0f1783c-fdb1-481b-c316-63c8c9417063"
      },
      "source": [
        "from torchvision import models #pytorch 공식문서에서 확인 한 것처럼, 여기서 여러 모델을 불러올 수 있음\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "#resnet18/34/50 \n",
        "model = models.resnet50(pretrained=True) #pretrained=True로 설정하면 pre-trained model의 parameter값을 그대로 가져옴, False로 설정하면 모델의 아키텍처만 가져오고 parameter는 랜덤 설정\n",
        "num_ftrs = model.fc.in_features #모델의 마지막 레이어의 입력 채널의 수를 저장(in_features는 해당 레이어의 입력 채널 수를 의미)   \n",
        "model.fc = nn.Linear(num_ftrs, len(class_names)) #모델의 마지막 레이어를 새로운 레이어로 교체 (입력 채널 수는 기존 레이어와 동일, 출력 채널 수를 우리가 원하는 수로 설정하는 것! 여기서는 클래스 수 12개) \n",
        "\n",
        "'''\n",
        "#vgg16/19\n",
        "model = models.vgg16(pretrained=True)\n",
        "#model.classifier[6].out_features = len(class_names) #마지막 레이어를 교체하는 방법이 약간 다름, print 해서 구조 확인하면서 이해\n",
        "\n",
        "#mobilenet_v2\n",
        "model = models.mobilenet_v2(pretrained=True)\n",
        "#model.classifier[1].out_features = len(class_names)\n",
        "\n",
        "#mobilnet_v3_small\n",
        "model = models.mobilenet_v3_small(pretrained=True)\n",
        "#model.classifier[3].out_features = len(class_names)\n",
        "'''\n",
        "\n",
        "model = model.to(DEVICE) #모델 gpu에 태우기\n",
        "print(model)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=2048, out_features=12, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Layer Freeze"
      ],
      "metadata": {
        "id": "4zzyFflRf13T"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wf8IIPgs3vs"
      },
      "source": [
        "cnt = 0 #몇 번째 Layer인지 나타내는 변수 cnt 설정\n",
        "for child in model.children(): #모델의 모든 Layer 정보를 담고 있음 (vgg, mobilenet 계열은 model.features)\n",
        "    cnt += 1 \n",
        "    #import pdb;pdb.set_trace()\n",
        "    if cnt < 8: #resnet50기준 10개의 Layer중 1~5개는 Freeze하고, 6~10은 학습 시 parameter를 업데이트 하도록!\n",
        "        #print(child)\n",
        "        for param in child.parameters(): #vgg, mobilenet 계열은 model.features.parameters()\n",
        "            param.requires_grad = False  #False -> NO UPDATE(FREEZE), True -> UPDATE(기본값)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. 손실함수, 최적화함수, 스케쥴러 설정\n",
        "- Adam vs SGD\n",
        "- learning rate는 작게!\n",
        "- 미리 학습 코드까지 실행!"
      ],
      "metadata": {
        "id": "onKCFqbZf9oZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 학습에 사용하는 Loss 함수를 지정\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#Optimizer는 Adam->SGD로 바꿈, filter와 lambda를 사용하는 이유 : param.requires_grad = True로 설정된 Layer의 parameter만을 업데이트 하기 위해서!\n",
        "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001) \n",
        " \n",
        "from torch.optim import lr_scheduler\n",
        "# 에포크에 따라 Learning Rate를 변경하는 역할 (7 에포크마다 0.1씩 곱해 LR을 감소시킴), Why? : 학습 보폭을 정하는 일은 매우 중요한데, 처음엔 크게 -> 학습 진행될 수록 작게 설정하는 것이 좋다고 알려짐, but 아직 연구중\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "metadata": {
        "id": "LfwDUXcaD_uD"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. 모델 학습 및 저장"
      ],
      "metadata": {
        "id": "t86IqtKnK8Qr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXFjVMs3s5Jv"
      },
      "source": [
        "# 전이학습 모델 학습 및 검증\n",
        "import time\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n",
        "    \n",
        "    train_losses , train_accuracy = [],[] #그래프 그리기 위해서 train에 대한 loss,accuracy 저장\n",
        "    val_losses , val_accuracy = [],[] #그래프 그리기 위해서 validation에 대한 loss,accuracy 저장\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())  #정확도가 가장 높은 모델을 저장\n",
        "    best_acc = 0.0 #정확도가 가장 높은 모델의 정확도 저장\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print('-------------- epoch {} ----------------'.format(epoch+1)) \n",
        "        since = time.time() #한 에포크 돌 때 소요되는 시간 측정(시작 시각 저장)                                    \n",
        "        for phase in ['train', 'val']: #한 에포크 돌 때 train 한 번, validation 한 번씩 각각 진행\n",
        "            if phase == 'train': \n",
        "                model.train() #train이면 학습 모드\n",
        "            else:\n",
        "                model.eval() #validation이면 평가 모드(평가 때 사용하지 말아야 할 작업들 알아서 꺼줌, dropout이나 batchnorm layer 같은 것들)     \n",
        " \n",
        "            running_loss = 0.0   #모든 데이터의 loss를 합해서 저장\n",
        "            running_corrects = 0 #정확하게 예측한 경우의 수를 저장\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]: #모델의 현재 모드(train or validation)에 해당하는 Dataloader에서 데이터를 받는 for문\n",
        "                inputs = inputs.to(DEVICE) #데이터를 gpu에 태움 \n",
        "                labels = labels.to(DEVICE) #데이터의 라벨값을 gpu에 태움\n",
        "                \n",
        "                optimizer.zero_grad() #학습 진행하면 이전 Batch의 Gradient값이 Optimizer에 저장될 것이므로 초기화 해주고 시작해야 함\n",
        "                \n",
        "                with torch.set_grad_enabled(phase == 'train'): #set_grad_enabled를 이용하면 train 모드에서만 모델의 Gradient를 업데이트 하도록 설정 할 수 있음\n",
        "                    outputs = model(inputs) #드디어 데이터를 모델에 입력!\n",
        "                    _, preds = torch.max(outputs, 1) #모델에 입력된 데이터가 12개의 클래스에 속할 확률값 출력, 이 중 가장 높은 값의 인덱스를 예측값으로 preds에 저장\n",
        "                    loss = criterion(outputs, labels) #모델의 예측값과 정답값 사이의 Loss를 계산(criterion 함수는 위에서 미리 설정해 둔 것)\n",
        "    \n",
        "                    if phase == 'train':   \n",
        "                        loss.backward() #계산한 loss값을 이용하여 BackPropagation을 통해 계산한 Gradient값을 parameter에 할당하고,\n",
        "                        optimizer.step() #모델의 parameter 업데이트\n",
        " \n",
        "                running_loss += loss.item() * inputs.size(0) #모든 데이터의 loss를 합해서 저장하기 위해, 하나의 미니 배치에 대한 loss값에 데이터의 수를 곱해서 더함 (inputs.size(0)이 미니 배치의 수) \n",
        "                running_corrects += torch.sum(preds == labels.data) #예측값과 정답값이 같으면 증가!\n",
        "\n",
        "            if phase == 'train':  \n",
        "                scheduler.step() #위에서 미리 설정한 Scheduler 실행\n",
        " \n",
        "            epoch_loss = running_loss/dataset_sizes[phase] #해당 에포크의 loss를 계산하기 위해 running_loss를 데이터셋 사이즈로 나눔\n",
        "            epoch_acc = running_corrects.double()/dataset_sizes[phase] #정확도도 마찬가지로 running_corrects를 데이터셋 사이즈로 나눔\n",
        " \n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc)) #해당 에포크의 loss와 정확도를 매번 출력\n",
        "\n",
        "            if phase == 'train': #그래프 그리기 위해 train 데이터의 loss와 accuracy 따로 저장\n",
        "                train_losses.append(epoch_loss)\n",
        "                train_accuracy.append(epoch_acc)\n",
        "            if phase == 'val': #그래프 그리기 위해 \bvalidation 데이터의 loss와 accuracy 따로 저장\n",
        "                val_losses.append(epoch_loss)\n",
        "                val_accuracy.append(epoch_acc)\n",
        "          \n",
        "            if phase == 'val' and epoch_acc > best_acc: #validation 모드에서 정확도가 최고 정확도 보다 높으면 업데이트\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict()) #최고 정확도를 가진 모델을 best_model_wts 변수에 저장\n",
        " \n",
        "        time_elapsed = time.time() - since #한 에포크 돌 때 소요되는 시간 측정(종료 시각 - 시작 시각) \n",
        "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) #계산한 시간 분과 초로 출력\n",
        "\n",
        "    #학습 종료 후 \n",
        "    print('Best validation Acc: {:4f}'.format(best_acc)) #validation 중 최고 정확도 출력\n",
        "\n",
        "    #train과 validation의 loss, accuracy 그래프 출력 -> 과적합 여부 등 판단\n",
        "    plt.plot(range(1,len(train_losses)+1),train_losses,'bo',label = 'training loss')\n",
        "    plt.plot(range(1,len(val_losses)+1),val_losses,'r',label = 'validation loss')\n",
        "    plt.legend()\n",
        "    plt.plot(range(1,len(train_accuracy)+1),train_accuracy,'co',label = 'training accuracy')\n",
        "    plt.plot(range(1,len(val_accuracy)+1),val_accuracy,'m',label = 'validation accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    #정확도가 가장 높았던 모델을 불러와서 반환\n",
        "    model.load_state_dict(best_model_wts) \n",
        "    return model"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        },
        "id": "EQ6wBtMAs6pw",
        "outputId": "b63e8e27-09da-4a98-ee8b-4522d0180eea"
      },
      "source": [
        "# 전이학습 실행\n",
        "model = train_model(model, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=EPOCH) \n",
        "\n",
        "# 반환 받은 정확도가 가장 높았던 모델을 torch.save 이용해서 저장 (모델 별로 이름 변경해서 저장!)\n",
        "torch.save(model, '/content/drive/MyDrive/plant-leaf-dataset/resnet50.pt')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------- epoch 1 ----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 1.2346 Acc: 0.6821\n",
            "val Loss: 0.5108 Acc: 0.8603\n",
            "Completed in 0m 30s\n",
            "-------------- epoch 2 ----------------\n",
            "train Loss: 0.2770 Acc: 0.9190\n",
            "val Loss: 0.1936 Acc: 0.9397\n",
            "Completed in 0m 29s\n",
            "-------------- epoch 3 ----------------\n",
            "train Loss: 0.1162 Acc: 0.9644\n",
            "val Loss: 0.0958 Acc: 0.9677\n",
            "Completed in 0m 30s\n",
            "-------------- epoch 4 ----------------\n",
            "train Loss: 0.0676 Acc: 0.9800\n",
            "val Loss: 0.0748 Acc: 0.9780\n",
            "Completed in 0m 29s\n",
            "-------------- epoch 5 ----------------\n",
            "train Loss: 0.0437 Acc: 0.9862\n",
            "val Loss: 0.0603 Acc: 0.9815\n",
            "Completed in 0m 29s\n",
            "Best validation Acc: 0.981466\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjkyMbUEuMqi"
      },
      "source": [
        "### 7. 모델 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPwYDjEHtCXm",
        "outputId": "17d0bc97-866b-4447-c308-69de86c95644"
      },
      "source": [
        "# 전이학습 평가 전처리 (위에서 설명한 것과 동일)\n",
        "data_transforms = transforms.Compose([ \n",
        "        transforms.Resize([64,64]),  \n",
        "        transforms.RandomCrop(52),  \n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
        "        ])\n",
        "\n",
        "#경로 맞춰서 변경해 주세요!\n",
        "test_dataset = ImageFolder(root='/content/drive/MyDrive/plant-leaf-dataset/plant-leaf-new-dataset/test', transform=data_transforms) \n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "# 모델 평가 함수\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval() #모델을 평가 모드로 설정\n",
        "    test_loss = 0 #미니 배치 별로 loss를 합산해서 저장\n",
        "    correct = 0 #정확하게 예측한 수 저장   \n",
        "    with torch.no_grad(): #해당 메서드를 이용해서 parameter 업데이트 방지\n",
        "        for data, target in test_loader:  \n",
        "            data, target = data.to(DEVICE), target.to(DEVICE) #데이터와 라벨을 불러오면서 gpu에 태움  \n",
        "            output = model(data) #데이터를 모델에 입력           \n",
        "            test_loss += torch.nn.functional.cross_entropy(output,target, reduction='sum').item() #모델의 예측값과 정답값 사이의 loss 계산\n",
        "            pred = output.max(1, keepdim=True)[1]  #모델에 입력된 데이터가 12개의 클래스에 속할 확률값 출력, 이 중 가장 높은 값의 인덱스를 예측값으로 pred에 저장\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item() #target.view_as(pred)를 이용해 target의 텐서 구조를 pred의 텐서와 같은 모양으로 재정렬 (모델 만들 때 쓰는 view와 비슷 view는 숫자 직접 지정)\n",
        "                                                                  #eq는 비교 연산자로 pred와 target.view_as(pred)의 값이 일치하면 1, 일치하지 않으면 0 반환\n",
        "   \n",
        "    test_loss /= len(test_loader.dataset) #모든 미니 배치에서 합한 loss값을 배치 수로 나누어 loss값의 평균 구함\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset) #마찬가지로 정확도의 평균도 구함\n",
        "    \n",
        "    return test_loss, test_accuracy #계산한 Test 데이터의 loss와 정확도 반환\n",
        "\n",
        "# 전이학습 모델 평가 결과\n",
        "model=torch.load('/content/drive/MyDrive/plant-leaf-dataset/resnet50.pt') #torch.load를 이용해서 원하는 모델 불러오기!\n",
        "test_loss, test_accuracy = evaluate(model, test_loader) #평가 함수 이용해서 Test 데이터에 대한 loss 및 정확도 측정\n",
        "print('model test acc:  ', test_accuracy) #평가 정확도 출력"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model test acc:   98.31896551724138\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(과제) 한 가지 이상의 변화를 준 후 학습을 돌려서 결과와 함께 간단한 설명을 업로드 해주세요 😀\n",
        "\n",
        "예시 : 다른 전이학습 모델 사용, freeze 시키는 구간 변화, 직접 짠 모델과의 성능 비교, 데이터 수의 변화, optimizer에 대한 실험, epoch 늘리기, 등등"
      ],
      "metadata": {
        "id": "maEk9ITatoai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#lr=0.00001-> 0.0001로 바꿈!"
      ],
      "metadata": {
        "id": "WWBfXLfafUu9"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_AlgCxq4fXmh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
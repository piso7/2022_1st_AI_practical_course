{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3주차(금)_CNN실습(박지하).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5dfb8ce71d0f434d96414a9d51147734": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_39bca8b3f88b45db8126982a307105d7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_29606b9086134a44b633d61ad5f92606",
              "IPY_MODEL_f8e8d7af371c408782dffd6e467daf46",
              "IPY_MODEL_1d80c77a2da14323850146864f665696"
            ]
          }
        },
        "39bca8b3f88b45db8126982a307105d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "29606b9086134a44b633d61ad5f92606": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_193fa2d8d7d34251ac11f256eebe5d06",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ad95585533cb425dbb3c0fea9d3f1271"
          }
        },
        "f8e8d7af371c408782dffd6e467daf46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0e861c47732148b79efa064198fda6a9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102530333,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102530333,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_58a82ec1fce64f859cffa0f58cd0b2cf"
          }
        },
        "1d80c77a2da14323850146864f665696": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3917b3c2f71d44a9b66936b7688c9578",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:03&lt;00:00, 41.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ccdefcba4e1f4bd2b6bbd1a3071ad087"
          }
        },
        "193fa2d8d7d34251ac11f256eebe5d06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ad95585533cb425dbb3c0fea9d3f1271": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0e861c47732148b79efa064198fda6a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "58a82ec1fce64f859cffa0f58cd0b2cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3917b3c2f71d44a9b66936b7688c9578": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ccdefcba4e1f4bd2b6bbd1a3071ad087": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 0. 데이터 불러오기\n",
        "https://drive.google.com/file/d/1M8KwdmGm8EWCn_IEWAcctbUJBww-M3cF/view?usp=sharing\n",
        "\n",
        "1. 위 링크에 있는 zip 파일을 '드라이브에 바로가기 추가'하기(안되면 그냥 다운로드 후 내 드라이브에 업로드)\n",
        "2. GPU 설정 후, 드라이브 마운트\n",
        "3. zip 파일 풀기 (약 2분 소요)"
      ],
      "metadata": {
        "id": "TDekbT7bHvKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2aBNcZVijGk",
        "outputId": "e07efecc-cb8e-45ba-d5d0-d6e8751bc337"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip -uq “압축을 풀 zip 파일의 경로” -d “압축을 풀고자 하는 폴더의 경로”\n",
        "!unzip -uq /content/drive/MyDrive/plant-leaf-dataset.zip -d /content/drive/MyDrive/plant-leaf-dataset"
      ],
      "metadata": {
        "id": "0CrELDhBI3yO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAmOFLpdtXV5"
      },
      "source": [
        "### 1. 데이터 분할을 위한 디렉토리 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH7lRtSlpG7c"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "original_dataset_dir = '/content/drive/MyDrive/plant-leaf-dataset/plant-leaf-dataset' #데이터셋이 위치한 경로 지정  \n",
        "classes_list = os.listdir(original_dataset_dir) #해당 경로 하위에 있는 모든 폴더의 목록을 가져옴(폴더 목록 == 클래스 목록)\n",
        " \n",
        "base_dir = '/content/drive/MyDrive/plant-leaf-dataset/plant-leaf-new-dataset' #train/val/test로 분할한 데이터를 저장할 폴더 생성\n",
        "os.mkdir(base_dir)\n",
        " \n",
        "train_dir = os.path.join(base_dir, 'train') #train 폴더 생성\n",
        "os.mkdir(train_dir)\n",
        "validation_dir = os.path.join(base_dir, 'val') #\bvalidation 폴더 생성\n",
        "os.mkdir(validation_dir)\n",
        "test_dir = os.path.join(base_dir, 'test') #test 폴더 생성\n",
        "os.mkdir(test_dir)\n",
        "\n",
        "for cls in classes_list: #train/val/test 폴더에 각각 클래스 목록 폴더를 생성    \n",
        "    os.mkdir(os.path.join(train_dir, cls))\n",
        "    os.mkdir(os.path.join(validation_dir, cls))\n",
        "    os.mkdir(os.path.join(test_dir, cls))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. train/validation/test 데이터 분할 및 클래스 별 데이터 수 확인"
      ],
      "metadata": {
        "id": "eKJ1QY2e28i4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_v0a0PUSrdnZ",
        "outputId": "7e4130f6-ae53-46a2-d1e1-fc1dea97925c"
      },
      "source": [
        "import math\n",
        "for cls in classes_list: #모든 클래스에 대한 작업 반복\n",
        "    path = os.path.join(original_dataset_dir, cls) \n",
        "    fnames = os.listdir(path) #path 위치에 존재하는 모든 이미지 파일의 목록을 fnames에 저장\n",
        "    \n",
        "    #train/validation/test 의 비율을 6:2:2로 (데이터 규모에 따라 조정 가능)\n",
        "    train_size = math.floor(len(fnames) * 0.65)\n",
        "    validation_size = math.floor(len(fnames) * 0.15)\n",
        "    test_size = math.floor(len(fnames) * 0.2)\n",
        "    \n",
        "    #train\n",
        "    train_fnames = fnames[:train_size] #train 데이터에 해당하는 파일의 이름을 train_fnames에 저장\n",
        "    for fname in train_fnames: #train 데이터에 대해 for문의 내용 반복\n",
        "        src = os.path.join(path, fname) #복사할 원본 파일의 경로 지정\n",
        "        dst = os.path.join(os.path.join(train_dir, cls), fname) #복사한 후 저장할 파일의 경로 지정\n",
        "        shutil.copyfile(src, dst) #src의 경로에 해당하는 파일을 dst의 경로에 지정\n",
        "    \n",
        "    #validation\n",
        "    validation_fnames = fnames[train_size:(validation_size + train_size)]\n",
        "    for fname in validation_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(validation_dir, cls), fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "        \n",
        "    #test    \n",
        "    test_fnames = fnames[(train_size + validation_size):(test_size + validation_size + train_size)]\n",
        "    for fname in test_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(test_dir, cls), fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "\n",
        "    print(\"class(\",cls,\") Train:\",len(train_fnames), \"Validation:\",len(validation_fnames), \"Test:\",len(test_fnames))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class( Apple___healthy ) Train: 1069 Validation: 246 Test: 329\n",
            "class( Grape___healthy ) Train: 274 Validation: 63 Test: 84\n",
            "class( Grape___Black_rot ) Train: 767 Validation: 177 Test: 236\n",
            "class( Peach___Bacterial_spot ) Train: 1493 Validation: 344 Test: 459\n",
            "class( Potato___healthy ) Train: 98 Validation: 22 Test: 30\n",
            "class( Potato___Early_blight ) Train: 650 Validation: 150 Test: 200\n",
            "class( Corn___Common_rust ) Train: 774 Validation: 178 Test: 238\n",
            "class( Strawberry___Leaf_scorch ) Train: 727 Validation: 167 Test: 223\n",
            "class( Apple___Apple_scab ) Train: 409 Validation: 94 Test: 126\n",
            "class( Strawberry___healthy ) Train: 296 Validation: 68 Test: 91\n",
            "class( Peach___healthy ) Train: 234 Validation: 54 Test: 72\n",
            "class( Corn___healthy ) Train: 755 Validation: 174 Test: 232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYCY0sqFso7L"
      },
      "source": [
        "### 3. 기본 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucURIVBmsnmC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef2e36ae-a882-42ed-9be6-0c20c088433b"
      },
      "source": [
        "import torch\n",
        "import os\n",
        " \n",
        "USE_CUDA = torch.cuda.is_available() #GPU 사용 가능한지 확인하는 메서드(사용할 수 있으면 TRUE, 없으면 FALSE 반환)\n",
        "print(USE_CUDA)\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\") #DEVICE 변수에 TRUE 이면 cuda를 FALSE 이면 cpu를 저장\n",
        "print(DEVICE)\n",
        "\n",
        "BATCH_SIZE = 256 #배치사이즈 지정\n",
        "EPOCH = 15 #에포크 지정\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "data_transforms = { # transforms.Compose()는 이미지 전처리, Augmentation 등 사용, Augmentation이란? 좌우 반전, 밝기 조절, 이미지 확대 등 노이즈를 주어 더 강한 모델을 만들어 주는 기법\n",
        "    'train': transforms.Compose([transforms.Resize([64,64]), # Resize -> 이미지의 크기를 64x64로 조정                    \n",
        "                                 transforms.RandomHorizontalFlip(), #RandomHorizontalFlip -> 이미지를 무작위로 좌우 반전\n",
        "                                 transforms.RandomVerticalFlip(), #RandomVerticalFlip -> 이미지를 무작위로 상하 반전\n",
        "                                 transforms.RandomCrop(52), #RandomCrop -> 이미지의 일부를 랜덤하게 잘라서 52x52 사이즈로 변경\n",
        "                                 transforms.ToTensor(), # ToTensor -> 이미지를 텐서 형태로 변환하고, 모든 값을 0~1 사이로 변경\n",
        "                                 transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), #Normalize ->정규화를 위해선 평균값과 표준편차값이 필요\n",
        "                                                                                                        #            첫번째[]는 R,G,B 채널 값에서 정규화를 적용할 평균값 \n",
        "                                                                                                        #            두번째[]는 R,G,B 채널 값에서 정규화를 적용할 표준편차값 \n",
        "                                                                                                        #            이 값은 이미지넷 데이터의 값이고, 정규화는 Local Minimum에 빠지는 것을 방지\n",
        "    'val': transforms.Compose([transforms.Resize([64,64]), \n",
        "                               #validation data는 Augmentation에 해당하는 부분을 제외하고 동일하게 전처리 \n",
        "                               transforms.RandomCrop(52), \n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ])\n",
        "}"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 데이터 로더"
      ],
      "metadata": {
        "id": "e0zmtPpS9oAW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STs5oRi2sy12",
        "outputId": "634d0d4c-45f3-42f6-9cac-98d411dca34b"
      },
      "source": [
        "from torchvision.datasets import ImageFolder #이미지 데이터는 하나의 클래스가 하나의 폴더에 대응되기 때문에 데이터셋을 불러올 때 ImageFolder를 사용\n",
        "\n",
        "# ImageFolder로 데이터셋 불러오기 -> root : 데이터 불러 올 경로 설정, transform : 앞서 설정한 전처리 방법 지정(불러오기 편하게 딕셔너리 형태로 구성)\n",
        "image_datasets = {x: ImageFolder(root=os.path.join(base_dir, x), transform=data_transforms[x]) for x in ['train', 'val']} \n",
        "\n",
        "# DataLoader로 불러온 이미지 데이터를 주어진 조건에 따라 미니 배치 단위로 분리 -> shuffle=True : 데이터의 순서가 섞여 학습시에 Label 정보의 순서를 기억하는 것을 방지 할 수 있음 필수!\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE, shuffle=True, num_workers=4) for x in ['train', 'val']} \n",
        "\n",
        "#train/validation의 총 개수를 저장\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "#12개 클래스의 목록을 저장\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "print(class_names)\n",
        "print(len(class_names))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Apple___Apple_scab', 'Apple___healthy', 'Corn___Common_rust', 'Corn___healthy', 'Grape___Black_rot', 'Grape___healthy', 'Peach___Bacterial_spot', 'Peach___healthy', 'Potato___Early_blight', 'Potato___healthy', 'Strawberry___Leaf_scorch', 'Strawberry___healthy']\n",
            "12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. 전이학습 모델 불러오기\n",
        "1. 모델만 불러와서 구조 print 해보기\n",
        "2. 분류층 바꾸고 print 해보기"
      ],
      "metadata": {
        "id": "Uy5j3kc79q6x"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZEFZgmTs2Vt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5dfb8ce71d0f434d96414a9d51147734",
            "39bca8b3f88b45db8126982a307105d7",
            "29606b9086134a44b633d61ad5f92606",
            "f8e8d7af371c408782dffd6e467daf46",
            "1d80c77a2da14323850146864f665696",
            "193fa2d8d7d34251ac11f256eebe5d06",
            "ad95585533cb425dbb3c0fea9d3f1271",
            "0e861c47732148b79efa064198fda6a9",
            "58a82ec1fce64f859cffa0f58cd0b2cf",
            "3917b3c2f71d44a9b66936b7688c9578",
            "ccdefcba4e1f4bd2b6bbd1a3071ad087"
          ]
        },
        "outputId": "21fc80f4-e992-4dc4-ff4b-69ad831a879b"
      },
      "source": [
        "from torchvision import models #pytorch 공식문서에서 확인 한 것처럼, 여기서 여러 모델을 불러올 수 있음\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "#resnet18/34/50 \n",
        "model = models.resnet50(pretrained=True) #pretrained=True로 설정하면 pre-trained model의 parameter값을 그대로 가져옴, False로 설정하면 모델의 아키텍처만 가져오고 parameter는 랜덤 설정\n",
        "num_ftrs = model.fc.in_features #모델의 마지막 레이어의 입력 채널의 수를 저장(in_features는 해당 레이어의 입력 채널 수를 의미)   \n",
        "model.fc = nn.Linear(num_ftrs, len(class_names)) #모델의 마지막 레이어를 새로운 레이어로 교체 (입력 채널 수는 기존 레이어와 동일, 출력 채널 수를 우리가 원하는 수로 설정하는 것! 여기서는 클래스 수 12개) \n",
        "\n",
        "'''\n",
        "#vgg16/19\n",
        "model = models.vgg16(pretrained=True)\n",
        "#model.classifier[6].out_features = len(class_names) #마지막 레이어를 교체하는 방법이 약간 다름, print 해서 구조 확인하면서 이해\n",
        "\n",
        "#mobilenet_v2\n",
        "model = models.mobilenet_v2(pretrained=True)\n",
        "#model.classifier[1].out_features = len(class_names)\n",
        "\n",
        "#mobilnet_v3_small\n",
        "model = models.mobilenet_v3_small(pretrained=True)\n",
        "#model.classifier[3].out_features = len(class_names)\n",
        "'''\n",
        "\n",
        "model = model.to(DEVICE) #모델 gpu에 태우기\n",
        "print(model)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5dfb8ce71d0f434d96414a9d51147734",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=2048, out_features=12, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Layer Freeze"
      ],
      "metadata": {
        "id": "4zzyFflRf13T"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wf8IIPgs3vs"
      },
      "source": [
        "cnt = 0 #몇 번째 Layer인지 나타내는 변수 cnt 설정\n",
        "for child in model.children(): #모델의 모든 Layer 정보를 담고 있음 (vgg, mobilenet 계열은 model.features)\n",
        "    cnt += 1 \n",
        "    if cnt < 7: #resnet50기준 10개의 Layer중 1~5개는 Freeze하고, 6~10은 학습 시 parameter를 업데이트 하도록!\n",
        "        #print(child)\n",
        "        for param in child.parameters(): #vgg, mobilenet 계열은 model.features.parameters()\n",
        "            param.requires_grad = False  #False -> NO UPDATE(FREEZE), True -> UPDATE(기본값)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. 손실함수, 최적화함수, 스케쥴러 설정\n",
        "- Adam vs SGD\n",
        "- learning rate는 작게!\n",
        "- 미리 학습 코드까지 실행!"
      ],
      "metadata": {
        "id": "onKCFqbZf9oZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 학습에 사용하는 Loss 함수를 지정\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#Optimizer는 Adam, filter와 lambda를 사용하는 이유 : param.requires_grad = True로 설정된 Layer의 parameter만을 업데이트 하기 위해서!\n",
        "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.00001) \n",
        " \n",
        "from torch.optim import lr_scheduler\n",
        "# 에포크에 따라 Learning Rate를 변경하는 역할 (7 에포크마다 0.1씩 곱해 LR을 감소시킴), Why? : 학습 보폭을 정하는 일은 매우 중요한데, 처음엔 크게 -> 학습 진행될 수록 작게 설정하는 것이 좋다고 알려짐, but 아직 연구중\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma=0.1)"
      ],
      "metadata": {
        "id": "LfwDUXcaD_uD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. 모델 학습 및 저장"
      ],
      "metadata": {
        "id": "t86IqtKnK8Qr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXFjVMs3s5Jv"
      },
      "source": [
        "# 전이학습 모델 학습 및 검증\n",
        "import time\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n",
        "    \n",
        "    train_losses , train_accuracy = [],[] #그래프 그리기 위해서 train에 대한 loss,accuracy 저장\n",
        "    val_losses , val_accuracy = [],[] #그래프 그리기 위해서 validation에 대한 loss,accuracy 저장\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())  #정확도가 가장 높은 모델을 저장\n",
        "    best_acc = 0.0 #정확도가 가장 높은 모델의 정확도 저장\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print('-------------- epoch {} ----------------'.format(epoch+1)) \n",
        "        since = time.time() #한 에포크 돌 때 소요되는 시간 측정(시작 시각 저장)                                    \n",
        "        for phase in ['train', 'val']: #한 에포크 돌 때 train 한 번, validation 한 번씩 각각 진행\n",
        "            if phase == 'train': \n",
        "                model.train() #train이면 학습 모드\n",
        "            else:\n",
        "                model.eval() #validation이면 평가 모드(평가 때 사용하지 말아야 할 작업들 알아서 꺼줌, dropout이나 batchnorm layer 같은 것들)     \n",
        " \n",
        "            running_loss = 0.0   #모든 데이터의 loss를 합해서 저장\n",
        "            running_corrects = 0 #정확하게 예측한 경우의 수를 저장\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]: #모델의 현재 모드(train or validation)에 해당하는 Dataloader에서 데이터를 받는 for문\n",
        "                inputs = inputs.to(DEVICE) #데이터를 gpu에 태움 \n",
        "                labels = labels.to(DEVICE) #데이터의 라벨값을 gpu에 태움\n",
        "                \n",
        "                optimizer.zero_grad() #학습 진행하면 이전 Batch의 Gradient값이 Optimizer에 저장될 것이므로 초기화 해주고 시작해야 함\n",
        "                \n",
        "                with torch.set_grad_enabled(phase == 'train'): #set_grad_enabled를 이용하면 train 모드에서만 모델의 Gradient를 업데이트 하도록 설정 할 수 있음\n",
        "                    outputs = model(inputs) #드디어 데이터를 모델에 입력!\n",
        "                    _, preds = torch.max(outputs, 1) #모델에 입력된 데이터가 12개의 클래스에 속할 확률값 출력, 이 중 가장 높은 값의 인덱스를 예측값으로 preds에 저장\n",
        "                    loss = criterion(outputs, labels) #모델의 예측값과 정답값 사이의 Loss를 계산(criterion 함수는 위에서 미리 설정해 둔 것)\n",
        "    \n",
        "                    if phase == 'train':   \n",
        "                        loss.backward() #계산한 loss값을 이용하여 BackPropagation을 통해 계산한 Gradient값을 parameter에 할당하고,\n",
        "                        optimizer.step() #모델의 parameter 업데이트\n",
        " \n",
        "                running_loss += loss.item() * inputs.size(0) #모든 데이터의 loss를 합해서 저장하기 위해, 하나의 미니 배치에 대한 loss값에 데이터의 수를 곱해서 더함 (inputs.size(0)이 미니 배치의 수) \n",
        "                running_corrects += torch.sum(preds == labels.data) #예측값과 정답값이 같으면 증가!\n",
        "\n",
        "            if phase == 'train':  \n",
        "                scheduler.step() #위에서 미리 설정한 Scheduler 실행\n",
        " \n",
        "            epoch_loss = running_loss/dataset_sizes[phase] #해당 에포크의 loss를 계산하기 위해 running_loss를 데이터셋 사이즈로 나눔\n",
        "            epoch_acc = running_corrects.double()/dataset_sizes[phase] #정확도도 마찬가지로 running_corrects를 데이터셋 사이즈로 나눔\n",
        " \n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc)) #해당 에포크의 loss와 정확도를 매번 출력\n",
        "\n",
        "            if phase == 'train': #그래프 그리기 위해 train 데이터의 loss와 accuracy 따로 저장\n",
        "                train_losses.append(epoch_loss)\n",
        "                train_accuracy.append(epoch_acc)\n",
        "            if phase == 'val': #그래프 그리기 위해 \bvalidation 데이터의 loss와 accuracy 따로 저장\n",
        "                val_losses.append(epoch_loss)\n",
        "                val_accuracy.append(epoch_acc)\n",
        "          \n",
        "            if phase == 'val' and epoch_acc > best_acc: #validation 모드에서 정확도가 최고 정확도 보다 높으면 업데이트\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict()) #최고 정확도를 가진 모델을 best_model_wts 변수에 저장\n",
        " \n",
        "        time_elapsed = time.time() - since #한 에포크 돌 때 소요되는 시간 측정(종료 시각 - 시작 시각) \n",
        "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) #계산한 시간 분과 초로 출력\n",
        "\n",
        "    #학습 종료 후 \n",
        "    print('Best validation Acc: {:4f}'.format(best_acc)) #validation 중 최고 정확도 출력\n",
        "\n",
        "    #train과 validation의 loss, accuracy 그래프 출력 -> 과적합 여부 등 판단\n",
        "    plt.plot(range(1,len(train_losses)+1),train_losses,'bo',label = 'training loss')\n",
        "    plt.plot(range(1,len(val_losses)+1),val_losses,'r',label = 'validation loss')\n",
        "    plt.legend()\n",
        "    plt.plot(range(1,len(train_accuracy)+1),train_accuracy,'co',label = 'training accuracy')\n",
        "    plt.plot(range(1,len(val_accuracy)+1),val_accuracy,'m',label = 'validation accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    #정확도가 가장 높았던 모델을 불러와서 반환\n",
        "    model.load_state_dict(best_model_wts) \n",
        "    return model"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EQ6wBtMAs6pw",
        "outputId": "d70ab324-d431-4e82-d192-b57a2b94c988"
      },
      "source": [
        "# 전이학습 실행\n",
        "model = train_model(model, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=EPOCH) \n",
        "\n",
        "# 반환 받은 정확도가 가장 높았던 모델을 torch.save 이용해서 저장 (모델 별로 이름 변경해서 저장!)\n",
        "torch.save(model, '/content/drive/MyDrive/plant-leaf-dataset/resnet50.pt')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------- epoch 1 ----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 2.1169 Acc: 0.3855\n",
            "val Loss: 1.7029 Acc: 0.6339\n",
            "Completed in 0m 35s\n",
            "-------------- epoch 2 ----------------\n",
            "train Loss: 1.3872 Acc: 0.7320\n",
            "val Loss: 1.0689 Acc: 0.7881\n",
            "Completed in 0m 34s\n",
            "-------------- epoch 3 ----------------\n",
            "train Loss: 0.8819 Acc: 0.8126\n",
            "val Loss: 0.6948 Acc: 0.8359\n",
            "Completed in 0m 34s\n",
            "-------------- epoch 4 ----------------\n",
            "train Loss: 0.5947 Acc: 0.8565\n",
            "val Loss: 0.4751 Acc: 0.8780\n",
            "Completed in 0m 34s\n",
            "-------------- epoch 5 ----------------\n",
            "train Loss: 0.4107 Acc: 0.9039\n",
            "val Loss: 0.3445 Acc: 0.9223\n",
            "Completed in 0m 34s\n",
            "-------------- epoch 6 ----------------\n",
            "train Loss: 0.3346 Acc: 0.9275\n",
            "val Loss: 0.3276 Acc: 0.9280\n",
            "Completed in 0m 33s\n",
            "-------------- epoch 7 ----------------\n",
            "train Loss: 0.3194 Acc: 0.9294\n",
            "val Loss: 0.3101 Acc: 0.9303\n",
            "Completed in 0m 34s\n",
            "-------------- epoch 8 ----------------\n",
            "train Loss: 0.3112 Acc: 0.9321\n",
            "val Loss: 0.3031 Acc: 0.9269\n",
            "Completed in 0m 34s\n",
            "-------------- epoch 9 ----------------\n",
            "train Loss: 0.2996 Acc: 0.9371\n",
            "val Loss: 0.2859 Acc: 0.9453\n",
            "Completed in 0m 33s\n",
            "-------------- epoch 10 ----------------\n",
            "train Loss: 0.2908 Acc: 0.9381\n",
            "val Loss: 0.2767 Acc: 0.9390\n",
            "Completed in 0m 34s\n",
            "-------------- epoch 11 ----------------\n",
            "train Loss: 0.2914 Acc: 0.9329\n",
            "val Loss: 0.2761 Acc: 0.9396\n",
            "Completed in 0m 34s\n",
            "-------------- epoch 12 ----------------\n",
            "train Loss: 0.2828 Acc: 0.9402\n",
            "val Loss: 0.2750 Acc: 0.9407\n",
            "Completed in 0m 35s\n",
            "-------------- epoch 13 ----------------\n",
            "train Loss: 0.2823 Acc: 0.9424\n",
            "val Loss: 0.2786 Acc: 0.9361\n",
            "Completed in 0m 35s\n",
            "-------------- epoch 14 ----------------\n",
            "train Loss: 0.2860 Acc: 0.9384\n",
            "val Loss: 0.2786 Acc: 0.9361\n",
            "Completed in 0m 33s\n",
            "-------------- epoch 15 ----------------\n",
            "train Loss: 0.2870 Acc: 0.9363\n",
            "val Loss: 0.2745 Acc: 0.9355\n",
            "Completed in 0m 34s\n",
            "Best validation Acc: 0.945308\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjkyMbUEuMqi"
      },
      "source": [
        "### 7. 모델 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPwYDjEHtCXm",
        "outputId": "fad5f521-a108-413c-f692-d7761952bfe0"
      },
      "source": [
        "# 전이학습 평가 전처리 (위에서 설명한 것과 동일)\n",
        "data_transforms = transforms.Compose([ \n",
        "        transforms.Resize([64,64]),  \n",
        "        transforms.RandomCrop(52),  \n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
        "        ])\n",
        "\n",
        "#경로 맞춰서 변경해 주세요!\n",
        "test_dataset = ImageFolder(root='/content/drive/MyDrive/plant-leaf-dataset/plant-leaf-new-dataset/test', transform=data_transforms) \n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "# 모델 평가 함수\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval() #모델을 평가 모드로 설정\n",
        "    test_loss = 0 #미니 배치 별로 loss를 합산해서 저장\n",
        "    correct = 0 #정확하게 예측한 수 저장   \n",
        "    with torch.no_grad(): #해당 메서드를 이용해서 parameter 업데이트 방지\n",
        "        for data, target in test_loader:  \n",
        "            data, target = data.to(DEVICE), target.to(DEVICE) #데이터와 라벨을 불러오면서 gpu에 태움  \n",
        "            output = model(data) #데이터를 모델에 입력           \n",
        "            test_loss += torch.nn.functional.cross_entropy(output,target, reduction='sum').item() #모델의 예측값과 정답값 사이의 loss 계산\n",
        "            pred = output.max(1, keepdim=True)[1]  #모델에 입력된 데이터가 12개의 클래스에 속할 확률값 출력, 이 중 가장 높은 값의 인덱스를 예측값으로 pred에 저장\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item() #target.view_as(pred)를 이용해 target의 텐서 구조를 pred의 텐서와 같은 모양으로 재정렬 (모델 만들 때 쓰는 view와 비슷 view는 숫자 직접 지정)\n",
        "                                                                  #eq는 비교 연산자로 pred와 target.view_as(pred)의 값이 일치하면 1, 일치하지 않으면 0 반환\n",
        "   \n",
        "    test_loss /= len(test_loader.dataset) #모든 미니 배치에서 합한 loss값을 배치 수로 나누어 loss값의 평균 구함\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset) #마찬가지로 정확도의 평균도 구함\n",
        "    \n",
        "    return test_loss, test_accuracy #계산한 Test 데이터의 loss와 정확도 반환\n",
        "\n",
        "# 전이학습 모델 평가 결과\n",
        "model=torch.load('/content/drive/MyDrive/plant-leaf-dataset/resnet50.pt') #torch.load를 이용해서 원하는 모델 불러오기!\n",
        "test_loss, test_accuracy = evaluate(model, test_loader) #평가 함수 이용해서 Test 데이터에 대한 loss 및 정확도 측정\n",
        "print('model test acc:  ', test_accuracy) #평가 정확도 출력"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model test acc:   93.57758620689656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(과제) 한 가지 이상의 변화를 준 후 학습을 돌려서 결과와 함께 간단한 설명을 업로드 해주세요 😀\n",
        "\n",
        "예시 : 다른 전이학습 모델 사용, freeze 시키는 구간 변화, 직접 짠 모델과의 성능 비교, 데이터 수의 변화, optimizer에 대한 실험, epoch 늘리기, 등등"
      ],
      "metadata": {
        "id": "maEk9ITatoai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "< 결론 >\n",
        "\n",
        "1. Train 데이터의 비율을 늘리고, validation 데이터의 비율을 줄였습니다.\n",
        "2. 배치사이즈를 256으로 줄이고, epoch 값을 15로 늘렸습니다. 배치사이즈를 더 낮추거나 epoch 값을 더 늘리기에는, 학습 시간이 너무 오래 걸리는 문제가 생깁니다.\n",
        "3. 전이학습 모델은 resnet50으로 그대로 사용하되, freeze하는 layer를 하나 더 늘렸습니다.(6번째 layer까지 freeze)\n",
        "4. Scheduler의 step_size 값을 5로 낮추어 더 자주 learning rate가 조정될 수 있도록 설정해 주었습니다."
      ],
      "metadata": {
        "id": "Ppa1IE7DkBQi"
      }
    }
  ]
}
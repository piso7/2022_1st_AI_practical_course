{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SDH_3주차(금)_CNN실습(공유).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "65c44ab988224a1d8c1b9c13e2dfd0af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_193385a6863a4b4fb5229dcbe666e8ef",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9d3bea5a18764c32aff314ff22d0ed0e",
              "IPY_MODEL_a0844b353e024cee9eaf65d7fdd94d80",
              "IPY_MODEL_639123f326374dbca495483f4fb18f33"
            ]
          }
        },
        "193385a6863a4b4fb5229dcbe666e8ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9d3bea5a18764c32aff314ff22d0ed0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8dc0d89ee18a41b7bd99af915c9aec39",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1745e7af1230430585f7783a75730664"
          }
        },
        "a0844b353e024cee9eaf65d7fdd94d80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_11101ee8847c43c59cdf9deafdecedcd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102530333,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102530333,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fc23a91d1a554d0ea6483e2c6daa198e"
          }
        },
        "639123f326374dbca495483f4fb18f33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a38158ed24584c16b936a9e4519a25d1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:00&lt;00:00, 197MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a240b7935147407597d22c471c3bfb13"
          }
        },
        "8dc0d89ee18a41b7bd99af915c9aec39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1745e7af1230430585f7783a75730664": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "11101ee8847c43c59cdf9deafdecedcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fc23a91d1a554d0ea6483e2c6daa198e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a38158ed24584c16b936a9e4519a25d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a240b7935147407597d22c471c3bfb13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 0. 데이터 불러오기\n",
        "https://drive.google.com/file/d/1M8KwdmGm8EWCn_IEWAcctbUJBww-M3cF/view?usp=sharing\n",
        "\n",
        "1. 위 링크에 있는 zip 파일을 '드라이브에 바로가기 추가'하기(안되면 그냥 다운로드 후 내 드라이브에 업로드)\n",
        "2. GPU 설정 후, 드라이브 마운트\n",
        "3. zip 파일 풀기 (약 2분 소요)"
      ],
      "metadata": {
        "id": "TDekbT7bHvKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwB-eizjSbx_",
        "outputId": "e0bf7bbd-4aca-44e5-cb2d-d74c75dcc39e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip -uq “압축을 풀 zip 파일의 경로” -d “압축을 풀고자 하는 폴더의 경로”\n",
        "!unzip -uq /content/drive/MyDrive/plant-leaf-dataset.zip -d /content/drive/MyDrive/plant-leaf-dataset"
      ],
      "metadata": {
        "id": "0CrELDhBI3yO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAmOFLpdtXV5"
      },
      "source": [
        "### 1. 데이터 분할을 위한 디렉토리 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH7lRtSlpG7c"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "original_dataset_dir = '/content/drive/MyDrive/plant-leaf-dataset/plant-leaf-dataset' #데이터셋이 위치한 경로 지정  \n",
        "classes_list = os.listdir(original_dataset_dir) #해당 경로 하위에 있는 모든 폴더의 목록을 가져옴(폴더 목록 == 클래스 목록)\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/plant-leaf-dataset/plant-leaf-new-dataset' #train/val/test로 분할한 데이터를 저장할 폴더 생성\n",
        "os.mkdir(base_dir)\n",
        " \n",
        "train_dir = os.path.join(base_dir, 'train') #train 폴더 생성\n",
        "os.mkdir(train_dir)\n",
        "validation_dir = os.path.join(base_dir, 'val') #\bvalidation 폴더 생성\n",
        "os.mkdir(validation_dir)\n",
        "test_dir = os.path.join(base_dir, 'test') #test 폴더 생성\n",
        "os.mkdir(test_dir)\n",
        "\n",
        "for cls in classes_list: #train/val/test 폴더에 각각 클래스 목록 폴더를 생성    \n",
        "    os.mkdir(os.path.join(train_dir, cls))\n",
        "    os.mkdir(os.path.join(validation_dir, cls))\n",
        "    os.mkdir(os.path.join(test_dir, cls))\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. train/validation/test 데이터 분할 및 클래스 별 데이터 수 확인"
      ],
      "metadata": {
        "id": "eKJ1QY2e28i4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v0a0PUSrdnZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "064de7c0-e591-4e9a-f69d-f72c0a97b7f8"
      },
      "source": [
        "import math\n",
        "for cls in classes_list: #모든 클래스에 대한 작업 반복\n",
        "    path = os.path.join(original_dataset_dir, cls) \n",
        "    fnames = os.listdir(path) #path 위치에 존재하는 모든 이미지 파일의 목록을 fnames에 저장\n",
        "    \n",
        "    #train/validation/test 의 비율을 6:2:2로 (데이터 규모에 따라 조정 가능)\n",
        "    train_size = math.floor(len(fnames) * 0.6)\n",
        "    validation_size = math.floor(len(fnames) * 0.2)\n",
        "    test_size = math.floor(len(fnames) * 0.2)\n",
        "    \n",
        "    #train\n",
        "    train_fnames = fnames[:train_size] #train 데이터에 해당하는 파일의 이름을 train_fnames에 저장\n",
        "    for fname in train_fnames: #train 데이터에 대해 for문의 내용 반복\n",
        "        src = os.path.join(path, fname) #복사할 원본 파일의 경로 지정\n",
        "        dst = os.path.join(os.path.join(train_dir, cls), fname) #복사한 후 저장할 파일의 경로 지정\n",
        "        shutil.copyfile(src, dst) #src의 경로에 해당하는 파일을 dst의 경로에 지정\n",
        "    \n",
        "    #validation\n",
        "    validation_fnames = fnames[train_size:(validation_size + train_size)]\n",
        "    for fname in validation_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(validation_dir, cls), fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "        \n",
        "    #test    \n",
        "    test_fnames = fnames[(train_size + validation_size):(test_size + validation_size + train_size)]\n",
        "    for fname in test_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(test_dir, cls), fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "\n",
        "    print(\"class(\",cls,\") Train:\",len(train_fnames), \"Validation:\",len(validation_fnames), \"Test:\",len(test_fnames))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class( Apple___healthy ) Train: 987 Validation: 329 Test: 329\n",
            "class( Grape___healthy ) Train: 253 Validation: 84 Test: 84\n",
            "class( Grape___Black_rot ) Train: 708 Validation: 236 Test: 236\n",
            "class( Peach___Bacterial_spot ) Train: 1378 Validation: 459 Test: 459\n",
            "class( Potato___healthy ) Train: 91 Validation: 30 Test: 30\n",
            "class( Potato___Early_blight ) Train: 600 Validation: 200 Test: 200\n",
            "class( Corn___Common_rust ) Train: 715 Validation: 238 Test: 238\n",
            "class( Strawberry___Leaf_scorch ) Train: 671 Validation: 223 Test: 223\n",
            "class( Apple___Apple_scab ) Train: 378 Validation: 126 Test: 126\n",
            "class( Strawberry___healthy ) Train: 273 Validation: 91 Test: 91\n",
            "class( Peach___healthy ) Train: 216 Validation: 72 Test: 72\n",
            "class( Corn___healthy ) Train: 697 Validation: 232 Test: 232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYCY0sqFso7L"
      },
      "source": [
        "### 3. 기본 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucURIVBmsnmC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dd2241a-b3b1-4eb7-c0ce-f7344e52db82"
      },
      "source": [
        "import torch\n",
        "import os\n",
        " \n",
        "USE_CUDA = torch.cuda.is_available() #GPU 사용 가능한지 확인하는 메서드(사용할 수 있으면 TRUE, 없으면 FALSE 반환)\n",
        "print('USE_CUDA: ',USE_CUDA)\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\") #DEVICE 변수에 TRUE 이면 cuda를 FALSE 이면 cpu를 저장\n",
        "print('DEVICE: ',DEVICE)\n",
        "\n",
        "BATCH_SIZE = 512 # 배치사이즈 지정\n",
        "EPOCH = 20 # 에포크 지정 (최소 에포크 10 이상 설정해서 돌려보기)\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "data_transforms = { # transforms.Compose()는 이미지 전처리, Augmentation 등 사용, Augmentation이란? 좌우 반전, 밝기 조절, 이미지 확대 등 노이즈를 주어 더 강한 모델을 만들어 주는 기법\n",
        "    'train': transforms.Compose([transforms.Resize([64,64]), # Resize -> 이미지의 크기를 64x64로 조정\n",
        "                                 # augmentation -------------                 \n",
        "                                 transforms.RandomHorizontalFlip(), #RandomHorizontalFlip -> 이미지를 무작위로 좌우 반전\n",
        "                                 transforms.RandomVerticalFlip(), #RandomVerticalFlip -> 이미지를 무작위로 상하 반전\n",
        "                                 transforms.RandomCrop(52), #RandomCrop -> 이미지의 일부를 랜덤하게 잘라서 52x52 사이즈로 변경\n",
        "                                 # augmentation -------------\n",
        "                                 transforms.ToTensor(), # ToTensor -> 이미지를 텐서 형태로 변환하고, 모든 값을 0~1 사이로 변경\n",
        "                                 transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), #Normalize ->정규화를 위해선 평균값과 표준편차값이 필요\n",
        "                                                                                                        #            첫번째[]는 R,G,B 채널 값에서 정규화를 적용할 평균값\n",
        "                                                                                                        #            두번째[]는 R,G,B 채널 값에서 정규화를 적용할 표준편차값\n",
        "                                                                                                        #            이 값은 이미지넷 데이터의 값이고, 정규화는 Local Minimum에 빠지는 것을 방지\n",
        "    'val': transforms.Compose([transforms.Resize([64,64]), \n",
        "                               #validation data는 Augmentation에 해당하는 부분을 제외하고 동일하게 전처리 \n",
        "                               transforms.RandomCrop(52), \n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ])\n",
        "}"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "USE_CUDA:  True\n",
            "DEVICE:  cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 데이터 로더"
      ],
      "metadata": {
        "id": "e0zmtPpS9oAW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STs5oRi2sy12",
        "outputId": "28a52b6c-4da6-4bdd-f4ae-f6176b9989a6"
      },
      "source": [
        "from torchvision.datasets import ImageFolder #이미지 데이터는 하나의 클래스가 하나의 폴더에 대응되기 때문에 데이터셋을 불러올 때 ImageFolder를 사용\n",
        "\n",
        "# ImageFolder로 데이터셋 불러오기 \n",
        "# root : 데이터 불러 올 경로 설정\n",
        "# transform : 앞서 설정한 전처리 방법 지정(불러오기 편하게 딕셔너리 형태로 구성)\n",
        "image_datasets = {x: ImageFolder(root=os.path.join(base_dir, x), \n",
        "                                 transform=data_transforms[x]) for x in ['train', 'val']}\n",
        "\n",
        "# DataLoader로 불러온 이미지 데이터를 주어진 조건에 따라 미니 배치 단위로 분리\n",
        "# shuffle=True : 데이터의 순서가 섞여 학습시에 Label 정보의 순서를 기억하는 것을 방지 할 수 있음 (필수!)\n",
        "dataloaders = {x: torch.utils.data.DataLoader(\n",
        "    image_datasets[x], batch_size=BATCH_SIZE, shuffle=True, num_workers=4) for x in ['train', 'val']} \n",
        "\n",
        "#train/validation의 총 개수를 저장\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "#12개 클래스의 목록을 저장\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "print(class_names)\n",
        "print(len(class_names)) # 12"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Apple___Apple_scab', 'Apple___healthy', 'Corn___Common_rust', 'Corn___healthy', 'Grape___Black_rot', 'Grape___healthy', 'Peach___Bacterial_spot', 'Peach___healthy', 'Potato___Early_blight', 'Potato___healthy', 'Strawberry___Leaf_scorch', 'Strawberry___healthy']\n",
            "12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. 전이학습 모델 불러오기\n",
        "1. 모델만 불러와서 구조 print 해보기\n",
        "2. 분류층 바꾸고 print 해보기"
      ],
      "metadata": {
        "id": "Uy5j3kc79q6x"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZEFZgmTs2Vt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "65c44ab988224a1d8c1b9c13e2dfd0af",
            "193385a6863a4b4fb5229dcbe666e8ef",
            "9d3bea5a18764c32aff314ff22d0ed0e",
            "a0844b353e024cee9eaf65d7fdd94d80",
            "639123f326374dbca495483f4fb18f33",
            "8dc0d89ee18a41b7bd99af915c9aec39",
            "1745e7af1230430585f7783a75730664",
            "11101ee8847c43c59cdf9deafdecedcd",
            "fc23a91d1a554d0ea6483e2c6daa198e",
            "a38158ed24584c16b936a9e4519a25d1",
            "a240b7935147407597d22c471c3bfb13"
          ]
        },
        "outputId": "71accca2-ce90-495e-9adb-74a06c030f4e"
      },
      "source": [
        "from torchvision import models #pytorch 공식문서에서 확인 한 것처럼, 여기서 여러 모델을 불러올 수 있음\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "#resnet18/34/50 \n",
        "model = models.resnet50(pretrained=True) #pretrained=True로 설정하면 pre-trained model의 parameter값을 그대로 가져옴, False로 설정하면 모델의 아키텍처만 가져오고 parameter는 랜덤 설정\n",
        "num_ftrs = model.fc.in_features #모델의 마지막 레이어의 입력 채널의 수를 저장(in_features는 해당 레이어의 입력 채널 수를 의미)\n",
        "model.fc = nn.Linear(num_ftrs, len(class_names)) #모델의 마지막 레이어를 새로운 레이어로 교체 (입력 채널 수는 기존 레이어와 동일, 출력 채널 수를 우리가 원하는 수로 설정하는 것! 여기서는 클래스 수 12개)\n",
        "\n",
        "'''\n",
        "# resnet 이외의 다른 모델들\n",
        "#vgg16/19\n",
        "model = models.vgg16(pretrained=True)\n",
        "model.classifier[6].out_features = len(class_names) #마지막 레이어를 교체하는 방법이 약간 다름, print 해서 구조 확인하면서 이해\n",
        "\n",
        "#mobilenet_v2\n",
        "model = models.mobilenet_v2(pretrained=True)\n",
        "model.classifier[1].out_features = len(class_names)\n",
        "\n",
        "#mobilnet_v3_small\n",
        "model = models.mobilenet_v3_small(pretrained=True)\n",
        "model.classifier[3].out_features = len(class_names)\n",
        "'''\n",
        "\n",
        "model = model.to(DEVICE) #모델 gpu에 태우기\n",
        "print(model)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "65c44ab988224a1d8c1b9c13e2dfd0af",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=2048, out_features=12, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Layer Freeze"
      ],
      "metadata": {
        "id": "4zzyFflRf13T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if resnet\n",
        "\n",
        "cnt = 0 #몇 번째 Layer인지 나타내는 변수 cnt 설정\n",
        "for child in model.children(): #모델의 모든 Layer 정보를 담고 있음 (vgg, mobilenet 계열은 model.features)\n",
        "    cnt += 1 \n",
        "    if cnt < 6: #resnet50기준 10개의 Layer중 1~5개는 Freeze하고, 6~10은 학습 시 parameter를 업데이트 하도록!\n",
        "        #print(child)\n",
        "        for param in child.parameters(): #vgg, mobilenet 계열은 model.features.parameters()\n",
        "            param.requires_grad = False  #False -> NO UPDATE(FREEZE), True -> UPDATE(기본값)"
      ],
      "metadata": {
        "id": "pSuKfXMHdeL-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wf8IIPgs3vs"
      },
      "source": [
        "'''\n",
        "# vgg, mobilenet\n",
        "\n",
        "cnt = 0 #몇 번째 Layer인지 나타내는 변수 cnt 설정\n",
        "for feature in model.features: #모델의 모든 Layer 정보를 담고 있음 (vgg, mobilenet 계열은 model.children -> model.features)\n",
        "    cnt += 1\n",
        "    # import pdb : python debugger - n, q\n",
        "\n",
        "    if cnt < 6: #resnet50기준 10개의 Layer중 1~5개는 Freeze하고, 6~10은 학습 시 parameter를 업데이트 하도록!\n",
        "        #print(child)\n",
        "        for param in model.features.parameters(): #vgg, mobilenet 계열은 model.features.parameters() 코드 수정 필요 !\n",
        "            param.requires_grad = False  #False -> NO UPDATE(FREEZE), True -> UPDATE(기본값)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. 손실함수, 최적화함수, 스케쥴러 설정\n",
        "- Adam vs SGD\n",
        "- learning rate는 작게!\n",
        "- 미리 학습 코드까지 실행!"
      ],
      "metadata": {
        "id": "onKCFqbZf9oZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 학습에 사용하는 Loss 함수를 지정\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#Optimizer는 Adam, filter와 lambda를 사용하는 이유 : param.requires_grad = True로 설정된 Layer의 parameter만을 업데이트 하기 위해서!\n",
        "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.00001) # overfitting 방지 위해, lr값은 작게 설정하는게 좋다. \n",
        "\n",
        "from torch.optim import lr_scheduler\n",
        "# 에포크에 따라 Learning Rate를 변경하는 역할 (7 에포크마다 0.1씩 곱해 LR을 감소시킴), Why? : 학습 보폭을 정하는 일은 매우 중요한데, 처음엔 크게 -> 학습 진행될 수록 작게 설정하는 것이 좋다고 알려짐, but 아직 연구중\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "metadata": {
        "id": "LfwDUXcaD_uD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. 모델 학습 및 저장"
      ],
      "metadata": {
        "id": "t86IqtKnK8Qr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXFjVMs3s5Jv"
      },
      "source": [
        "# 전이학습 모델 학습 및 검증\n",
        "import time\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n",
        "    \n",
        "    train_losses , train_accuracy = [],[] #그래프 그리기 위해서 train에 대한 loss,accuracy 저장\n",
        "    val_losses , val_accuracy = [],[] #그래프 그리기 위해서 validation에 대한 loss,accuracy 저장\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())  #정확도가 가장 높은 모델을 저장\n",
        "    best_acc = 0.0 #정확도가 가장 높은 모델의 정확도 저장\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print('-------------- epoch {} ----------------'.format(epoch+1)) \n",
        "        since = time.time() #한 에포크 돌 때 소요되는 시간 측정(시작 시각 저장)                                    \n",
        "        for phase in ['train', 'val']: #한 에포크 돌 때 train 한 번, validation 한 번씩 각각 진행\n",
        "            if phase == 'train': \n",
        "                model.train() #train이면 학습 모드\n",
        "            else:\n",
        "                model.eval() #validation이면 평가 모드(평가 때 사용하지 말아야 할 작업들 알아서 꺼줌, dropout이나 batchnorm layer 같은 것들)\n",
        " \n",
        "            running_loss = 0.0   #모든 데이터의 loss를 합해서 저장\n",
        "            running_corrects = 0 #정확하게 예측한 경우의 수를 저장\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]: #모델의 현재 모드(train or validation)에 해당하는 Dataloader에서 데이터를 받는 for문\n",
        "                inputs = inputs.to(DEVICE) #데이터를 gpu에 태움 \n",
        "                labels = labels.to(DEVICE) #데이터의 라벨값을 gpu에 태움\n",
        "                \n",
        "                optimizer.zero_grad() #학습 진행하면 이전 Batch의 Gradient값이 Optimizer에 저장될 것이므로 초기화 해주고 시작해야 함\n",
        "                \n",
        "                with torch.set_grad_enabled(phase == 'train'): #set_grad_enabled를 이용하면 train 모드에서만 모델의 Gradient를 업데이트 하도록 설정 할 수 있음\n",
        "                    outputs = model(inputs) #드디어 데이터를 모델에 입력!\n",
        "                    _, preds = torch.max(outputs, 1) #모델에 입력된 데이터가 12개의 클래스에 속할 확률값 출력, 이 중 가장 높은 값의 인덱스를 예측값으로 preds에 저장\n",
        "                    loss = criterion(outputs, labels) #모델의 예측값과 정답값 사이의 Loss를 계산(criterion 함수는 위에서 미리 설정해 둔 것)\n",
        "    \n",
        "                    if phase == 'train':\n",
        "                        loss.backward() #계산한 loss값을 이용하여 BackPropagation을 통해 계산한 Gradient값을 parameter에 할당하고,\n",
        "                        optimizer.step() #모델의 parameter 업데이트\n",
        " \n",
        "                running_loss += loss.item() * inputs.size(0) #모든 데이터의 loss를 합해서 저장하기 위해, 하나의 미니 배치에 대한 loss값에 데이터의 수를 곱해서 더함 (inputs.size(0)이 미니 배치의 수) \n",
        "                running_corrects += torch.sum(preds == labels.data) #예측값과 정답값이 같으면 증가!\n",
        "\n",
        "            if phase == 'train':  \n",
        "                scheduler.step() #위에서 미리 설정한 Scheduler 실행\n",
        " \n",
        "            epoch_loss = running_loss/dataset_sizes[phase] #해당 에포크의 loss를 계산하기 위해 running_loss를 데이터셋 사이즈로 나눔\n",
        "            epoch_acc = running_corrects.double()/dataset_sizes[phase] #정확도도 마찬가지로 running_corrects를 데이터셋 사이즈로 나눔\n",
        " \n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc)) #해당 에포크의 loss와 정확도를 매번 출력\n",
        "\n",
        "            if phase == 'train': #그래프 그리기 위해 train 데이터의 loss와 accuracy 따로 저장\n",
        "                train_losses.append(epoch_loss)\n",
        "                train_accuracy.append(epoch_acc)\n",
        "            if phase == 'val': #그래프 그리기 위해 \bvalidation 데이터의 loss와 accuracy 따로 저장\n",
        "                val_losses.append(epoch_loss)\n",
        "                val_accuracy.append(epoch_acc)\n",
        "          \n",
        "            if phase == 'val' and epoch_acc > best_acc: #validation 모드에서 정확도가 최고 정확도 보다 높으면 업데이트\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict()) #최고 정확도를 가진 모델을 best_model_wts 변수에 저장\n",
        " \n",
        "        time_elapsed = time.time() - since #한 에포크 돌 때 소요되는 시간 측정(종료 시각 - 시작 시각) \n",
        "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) #계산한 시간 분과 초로 출력\n",
        "\n",
        "    #학습 종료 후 \n",
        "    print('Best validation Acc: {:4f}'.format(best_acc)) #validation 중 최고 정확도 출력\n",
        "\n",
        "    #train과 validation의 loss, accuracy 그래프 출력 -> 과적합 여부 등 판단\n",
        "    plt.plot(range(1,len(train_losses)+1),train_losses,'bo',label = 'training loss')\n",
        "    plt.plot(range(1,len(val_losses)+1),val_losses,'r',label = 'validation loss')\n",
        "    plt.legend()\n",
        "    plt.plot(range(1,len(train_accuracy)+1),train_accuracy,'co',label = 'training accuracy')\n",
        "    plt.plot(range(1,len(val_accuracy)+1),val_accuracy,'m',label = 'validation accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    #정확도가 가장 높았던 모델을 불러와서 반환\n",
        "    model.load_state_dict(best_model_wts) \n",
        "    return model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EQ6wBtMAs6pw",
        "outputId": "883e8148-05ef-4f9f-cd1c-a7e11226772f"
      },
      "source": [
        "# 전이학습 실행\n",
        "model = train_model(model, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=EPOCH)\n",
        "\n",
        "# 반환 받은 정확도가 가장 높았던 모델을 torch.save 이용해서 저장 (모델 별로 이름 변경해서 저장!)\n",
        "# torch.save(model, '/content/drive/MyDrive/plant-leaf-dataset/resnet50.pt')\n",
        "torch.save(model, '/content/drive/MyDrive/plant-leaf-dataset/resnet50.pt')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------- epoch 1 ----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.3879 Acc: 0.9011\n",
            "val Loss: 0.4012 Acc: 0.8957\n",
            "Completed in 0m 33s\n",
            "-------------- epoch 2 ----------------\n",
            "train Loss: 0.3781 Acc: 0.9053\n",
            "val Loss: 0.3980 Acc: 0.8935\n",
            "Completed in 0m 32s\n",
            "-------------- epoch 3 ----------------\n",
            "train Loss: 0.3821 Acc: 0.9018\n",
            "val Loss: 0.3962 Acc: 0.8948\n",
            "Completed in 0m 27s\n",
            "-------------- epoch 4 ----------------\n",
            "train Loss: 0.3794 Acc: 0.9031\n",
            "val Loss: 0.3993 Acc: 0.8948\n",
            "Completed in 0m 26s\n",
            "-------------- epoch 5 ----------------\n",
            "train Loss: 0.3858 Acc: 0.9010\n",
            "val Loss: 0.3951 Acc: 0.8940\n",
            "Completed in 0m 27s\n",
            "-------------- epoch 6 ----------------\n",
            "train Loss: 0.3868 Acc: 0.9024\n",
            "val Loss: 0.4019 Acc: 0.8922\n",
            "Completed in 0m 27s\n",
            "-------------- epoch 7 ----------------\n",
            "train Loss: 0.3846 Acc: 0.9025\n",
            "val Loss: 0.3981 Acc: 0.8927\n",
            "Completed in 0m 26s\n",
            "-------------- epoch 8 ----------------\n",
            "train Loss: 0.3798 Acc: 0.9046\n",
            "val Loss: 0.4017 Acc: 0.8970\n",
            "Completed in 0m 27s\n",
            "-------------- epoch 9 ----------------\n",
            "train Loss: 0.3817 Acc: 0.9043\n",
            "val Loss: 0.4046 Acc: 0.8922\n",
            "Completed in 0m 27s\n",
            "-------------- epoch 10 ----------------\n",
            "train Loss: 0.3824 Acc: 0.9031\n",
            "val Loss: 0.3926 Acc: 0.8983\n",
            "Completed in 0m 26s\n",
            "-------------- epoch 11 ----------------\n",
            "train Loss: 0.3791 Acc: 0.9063\n",
            "val Loss: 0.3974 Acc: 0.8983\n",
            "Completed in 0m 26s\n",
            "-------------- epoch 12 ----------------\n",
            "train Loss: 0.3840 Acc: 0.9051\n",
            "val Loss: 0.4026 Acc: 0.8961\n",
            "Completed in 0m 26s\n",
            "-------------- epoch 13 ----------------\n",
            "train Loss: 0.3803 Acc: 0.9048\n",
            "val Loss: 0.4000 Acc: 0.8961\n",
            "Completed in 0m 26s\n",
            "-------------- epoch 14 ----------------\n",
            "train Loss: 0.3778 Acc: 0.9043\n",
            "val Loss: 0.4004 Acc: 0.8974\n",
            "Completed in 0m 26s\n",
            "-------------- epoch 15 ----------------\n",
            "train Loss: 0.3764 Acc: 0.9073\n",
            "val Loss: 0.4024 Acc: 0.8987\n",
            "Completed in 0m 26s\n",
            "-------------- epoch 16 ----------------\n",
            "train Loss: 0.3790 Acc: 0.9074\n",
            "val Loss: 0.3973 Acc: 0.8927\n",
            "Completed in 0m 26s\n",
            "-------------- epoch 17 ----------------\n",
            "train Loss: 0.3833 Acc: 0.9015\n",
            "val Loss: 0.4013 Acc: 0.9000\n",
            "Completed in 0m 26s\n",
            "-------------- epoch 18 ----------------\n",
            "train Loss: 0.3814 Acc: 0.9067\n",
            "val Loss: 0.3922 Acc: 0.9009\n",
            "Completed in 0m 26s\n",
            "-------------- epoch 19 ----------------\n",
            "train Loss: 0.3809 Acc: 0.9063\n",
            "val Loss: 0.3966 Acc: 0.9022\n",
            "Completed in 0m 26s\n",
            "-------------- epoch 20 ----------------\n",
            "train Loss: 0.3785 Acc: 0.9054\n",
            "val Loss: 0.3937 Acc: 0.8974\n",
            "Completed in 0m 26s\n",
            "Best validation Acc: 0.902155\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjkyMbUEuMqi"
      },
      "source": [
        "### 7. 모델 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPwYDjEHtCXm",
        "outputId": "0d6a8c6d-507d-461d-d348-fca251e73d2f"
      },
      "source": [
        "# 전이학습 평가 전처리 (위에서 설명한 것과 동일)\n",
        "data_transforms = transforms.Compose([ \n",
        "        transforms.Resize([64,64]),  \n",
        "        transforms.RandomCrop(52),  \n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
        "        ])\n",
        "\n",
        "#경로 맞춰서 변경해 주세요!\n",
        "test_dataset = ImageFolder(root='/content/drive/MyDrive/plant-leaf-dataset/plant-leaf-new-dataset/test', transform=data_transforms) \n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "# 모델 평가 함수\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval() #모델을 평가 모드로 설정\n",
        "    test_loss = 0 #미니 배치 별로 loss를 합산해서 저장\n",
        "    correct = 0 #정확하게 예측한 수 저장   \n",
        "    with torch.no_grad(): #해당 메서드를 이용해서 parameter 업데이트 방지\n",
        "        for data, target in test_loader:  \n",
        "            data, target = data.to(DEVICE), target.to(DEVICE) #데이터와 라벨을 불러오면서 gpu에 태움  \n",
        "            output = model(data) #데이터를 모델에 입력           \n",
        "            test_loss += torch.nn.functional.cross_entropy(output,target, reduction='sum').item() #모델의 예측값과 정답값 사이의 loss 계산\n",
        "            pred = output.max(1, keepdim=True)[1]  #모델에 입력된 데이터가 12개의 클래스에 속할 확률값 출력, 이 중 가장 높은 값의 인덱스를 예측값으로 pred에 저장\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item() #target.view_as(pred)를 이용해 target의 텐서 구조를 pred의 텐서와 같은 모양으로 재정렬 (모델 만들 때 쓰는 view와 비슷 view는 숫자 직접 지정)\n",
        "                                                                  #eq는 비교 연산자로 pred와 target.view_as(pred)의 값이 일치하면 1, 일치하지 않으면 0 반환\n",
        "   \n",
        "    test_loss /= len(test_loader.dataset) #모든 미니 배치에서 합한 loss값을 배치 수로 나누어 loss값의 평균 구함\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset) #마찬가지로 정확도의 평균도 구함\n",
        "    \n",
        "    return test_loss, test_accuracy #계산한 Test 데이터의 loss와 정확도 반환\n",
        "\n",
        "# 전이학습 모델 평가 결과\n",
        "model=torch.load('/content/drive/MyDrive/plant-leaf-dataset/resnet50.pt') #torch.load를 이용해서 원하는 모델 불러오기!\n",
        "test_loss, test_accuracy = evaluate(model, test_loader) #평가 함수 이용해서 Test 데이터에 대한 loss 및 정확도 측정\n",
        "print('model test acc:  ', test_accuracy) #평가 정확도 출력\n",
        "# test acc: 83.44827586206897 ( default )\n",
        "# test acc: (epoch 10->20)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model test acc:   89.39655172413794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(과제) 한 가지 이상의 변화를 준 후 학습을 돌려서 결과와 함께 간단한 설명을 업로드 해주세요 😀\n",
        "\n",
        "예시 : 다른 전이학습 모델 사용, freeze 시키는 구간 변화, 직접 짠 모델과의 성능 비교, 데이터 수의 변화, optimizer에 대한 실험, epoch 늘리기, 등등"
      ],
      "metadata": {
        "id": "maEk9ITatoai"
      }
    }
  ]
}
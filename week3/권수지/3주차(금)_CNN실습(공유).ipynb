{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3주차(금)_CNN실습(공유).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 0. 데이터 불러오기\n",
        "https://drive.google.com/file/d/1M8KwdmGm8EWCn_IEWAcctbUJBww-M3cF/view?usp=sharing\n",
        "\n",
        "1. 위 링크에 있는 zip 파일을 '드라이브에 바로가기 추가'하기(안되면 그냥 다운로드 후 내 드라이브에 업로드)\n",
        "2. GPU 설정 후, 드라이브 마운트\n",
        "3. zip 파일 풀기 (약 2분 소요)"
      ],
      "metadata": {
        "id": "TDekbT7bHvKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "E1ielY3nSvoB",
        "outputId": "255a4559-1814-42d9-82ba-0d145b1cae81"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip -uq “압축을 풀 zip 파일의 경로” -d “압축을 풀고자 하는 폴더의 경로”\n",
        "!unzip -uq /content/drive/MyDrive/plant-leaf-dataset.zip -d /content/drive/MyDrive/plant-leaf-dataset"
      ],
      "metadata": {
        "id": "0CrELDhBI3yO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAmOFLpdtXV5"
      },
      "source": [
        "### 1. 데이터 분할을 위한 디렉토리 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH7lRtSlpG7c"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "original_dataset_dir = '/content/drive/MyDrive/plant-leaf-dataset/plant-leaf-dataset' #데이터셋이 위치한 경로 지정  \n",
        "classes_list = os.listdir(original_dataset_dir) #해당 경로 하위에 있는 모든 폴더의 목록을 가져옴(폴더 목록 == 클래스 목록)\n",
        " \n",
        "base_dir = '/content/drive/MyDrive/plant-leaf-dataset/plant-leaf-new-dataset' #train/val/test로 분할한 데이터를 저장할 폴더 생성\n",
        "os.mkdir(base_dir) #폴더 만드는 명령어\n",
        " \n",
        "train_dir = os.path.join(base_dir, 'train') #train 폴더 생성\n",
        "os.mkdir(train_dir)\n",
        "validation_dir = os.path.join(base_dir, 'val') #\bvalidation 폴더 생성\n",
        "os.mkdir(validation_dir)\n",
        "test_dir = os.path.join(base_dir, 'test') #test 폴더 생성\n",
        "os.mkdir(test_dir)\n",
        "\n",
        "for cls in classes_list: #train/val/test 폴더에 각각 클래스 목록 폴더를 생성    \n",
        "    os.mkdir(os.path.join(train_dir, cls))\n",
        "    os.mkdir(os.path.join(validation_dir, cls))\n",
        "    os.mkdir(os.path.join(test_dir, cls))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. train/validation/test 데이터 분할 및 클래스 별 데이터 수 확인"
      ],
      "metadata": {
        "id": "eKJ1QY2e28i4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_v0a0PUSrdnZ",
        "outputId": "0419d692-2ec9-4aa7-92ce-7af95ac21304"
      },
      "source": [
        "import math\n",
        "for cls in classes_list: #모든 클래스에 대한 작업 반복\n",
        "    path = os.path.join(original_dataset_dir, cls) \n",
        "    fnames = os.listdir(path) #path 위치에 존재하는 모든 이미지 파일의 목록을 fnames에 저장\n",
        "    \n",
        "    #train/validation/test 의 비율을 6:2:2로 (데이터 규모에 따라 조정 가능)\n",
        "    train_size = math.floor(len(fnames) * 0.6)\n",
        "    validation_size = math.floor(len(fnames) * 0.2)\n",
        "    test_size = math.floor(len(fnames) * 0.2)\n",
        "    \n",
        "    #train\n",
        "    train_fnames = fnames[:train_size] #train 데이터에 해당하는 파일의 이름을 train_fnames에 저장\n",
        "    for fname in train_fnames: #train 데이터에 대해 for문의 내용 반복\n",
        "        src = os.path.join(path, fname) #복사할 원본 파일의 경로 지정\n",
        "        dst = os.path.join(os.path.join(train_dir, cls), fname) #복사한 후 저장할 파일의 경로 지정\n",
        "        shutil.copyfile(src, dst) #src의 경로에 해당하는 파일을 dst의 경로에 지정\n",
        "    \n",
        "    #validation\n",
        "    validation_fnames = fnames[train_size:(validation_size + train_size)]\n",
        "    for fname in validation_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(validation_dir, cls), fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "        \n",
        "    #test    \n",
        "    test_fnames = fnames[(train_size + validation_size):(test_size + validation_size + train_size)]\n",
        "    for fname in test_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(test_dir, cls), fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "\n",
        "    print(\"class(\",cls,\") Train:\",len(train_fnames), \"Validation:\",len(validation_fnames), \"Test:\",len(test_fnames))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class( Apple___healthy ) Train: 987 Validation: 329 Test: 329\n",
            "class( Grape___healthy ) Train: 253 Validation: 84 Test: 84\n",
            "class( Grape___Black_rot ) Train: 708 Validation: 236 Test: 236\n",
            "class( Peach___Bacterial_spot ) Train: 1378 Validation: 459 Test: 459\n",
            "class( Potato___healthy ) Train: 91 Validation: 30 Test: 30\n",
            "class( Potato___Early_blight ) Train: 600 Validation: 200 Test: 200\n",
            "class( Corn___Common_rust ) Train: 715 Validation: 238 Test: 238\n",
            "class( Strawberry___Leaf_scorch ) Train: 671 Validation: 223 Test: 223\n",
            "class( Apple___Apple_scab ) Train: 378 Validation: 126 Test: 126\n",
            "class( Strawberry___healthy ) Train: 273 Validation: 91 Test: 91\n",
            "class( Peach___healthy ) Train: 216 Validation: 72 Test: 72\n",
            "class( Corn___healthy ) Train: 697 Validation: 232 Test: 232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYCY0sqFso7L"
      },
      "source": [
        "### 3. 기본 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucURIVBmsnmC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "be982f3b-e6fe-4421-aa75-e924d3753727"
      },
      "source": [
        "import torch\n",
        "import os\n",
        " \n",
        "USE_CUDA = torch.cuda.is_available() #GPU 사용 가능한지 확인하는 메서드(사용할 수 있으면 TRUE, 없으면 FALSE 반환)\n",
        "print(USE_CUDA)\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\") #DEVICE 변수에 TRUE 이면 cuda를 FALSE 이면 cpu를 저장\n",
        "print(DEVICE)\n",
        "\n",
        "BATCH_SIZE = 512 #배치사이즈 지정\n",
        "EPOCH = 10 #에포크 지정 #실습할때는 최소 에포크 10이상으로 설정해야함\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "data_transforms = { # transforms.Compose()는 이미지 전처리, Augmentation 등 사용, Augmentation이란? 좌우 반전, 밝기 조절, 이미지 확대 등 노이즈를 주어 더 강한 모델을 만들어 주는 기법\n",
        "    'train': transforms.Compose([transforms.Resize([64,64]), # Resize -> 이미지의 크기를 64x64로 조정                    \n",
        "                                 transforms.RandomHorizontalFlip(), #RandomHorizontalFlip -> 이미지를 무작위로 좌우 반전 오브멘테이션\n",
        "                                 transforms.RandomVerticalFlip(), #RandomVerticalFlip -> 이미지를 무작위로 상하 반전\n",
        "                                 transforms.RandomCrop(52), #RandomCrop -> 이미지의 일부를 랜덤하게 잘라서 52x52 사이즈로 변경\n",
        "                                 transforms.ToTensor(), # ToTensor -> 이미지를 텐서 형태로 변환하고, 모든 값을 0~1 사이로 변경\n",
        "                                 transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), #Normalize ->정규화를 위해선 평균값과 표준편차값이 필요\n",
        "                                                       #평균값               #표준편차값                #            첫번째[]는 R,G,B 채널 값에서 정규화를 적용할 평균값 \n",
        "                                                                                                        #            두번째[]는 R,G,B 채널 값에서 정규화를 적용할 표준편차값 \n",
        "                                                                                                        #            이 값은 이미지넷 데이터의 값이고, 정규화는 Local Minimum에 빠지는 것을 방지\n",
        "    'val': transforms.Compose([transforms.Resize([64,64]), \n",
        "                               #validation data는 Augmentation에 해당하는 부분을 제외하고 동일하게 전처리 \n",
        "                               transforms.RandomCrop(52), \n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ])\n",
        "}"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 데이터 로더"
      ],
      "metadata": {
        "id": "e0zmtPpS9oAW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "STs5oRi2sy12",
        "outputId": "ff5216a8-1b0e-4911-cd01-882b541da34b"
      },
      "source": [
        "from torchvision.datasets import ImageFolder #이미지 데이터는 하나의 클래스가 하나의 폴더에 대응되기 때문에 데이터셋을 불러올 때 ImageFolder를 사용\n",
        "\n",
        "# ImageFolder로 데이터셋 불러오기 -> root : 데이터 불러 올 경로 설정, transform : 앞서 설정한 전처리 방법 지정(불러오기 편하게 딕셔너리 형태로 구성)\n",
        "image_datasets = {x: ImageFolder(root=os.path.join(base_dir, x), transform=data_transforms[x]) for x in ['train', 'val']} \n",
        "\n",
        "# DataLoader로 불러온 이미지 데이터를 주어진 조건에 따라 미니 배치 단위로 분리 -> shuffle=True : 데이터의 순서가 섞여 학습시에 Label 정보의 순서를 기억하는 것을 방지 할 수 있음 필수!\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE, shuffle=True, num_workers=4) for x in ['train', 'val']} \n",
        "\n",
        "#train/validation의 총 개수를 저장\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "#12개 클래스의 목록을 저장\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "print(class_names)\n",
        "print(len(class_names))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Apple___Apple_scab', 'Apple___healthy', 'Corn___Common_rust', 'Corn___healthy', 'Grape___Black_rot', 'Grape___healthy', 'Peach___Bacterial_spot', 'Peach___healthy', 'Potato___Early_blight', 'Potato___healthy', 'Strawberry___Leaf_scorch', 'Strawberry___healthy']\n",
            "12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. 전이학습 모델 불러오기\n",
        "1. 모델만 불러와서 구조 print 해보기\n",
        "2. 분류층 바꾸고 print 해보기"
      ],
      "metadata": {
        "id": "Uy5j3kc79q6x"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZEFZgmTs2Vt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "eca4eadc-6d2d-46ee-ae93-3cb0a5c316d3"
      },
      "source": [
        "from torchvision import models #pytorch 공식문서에서 확인 한 것처럼, 여기서 여러 모델을 불러올 수 있음\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "#resnet18/34/50 #models.뒤에 내가 원하는 모델\n",
        "model = models.resnet50(pretrained=True) #pretrained=True로 설정하면 pre-trained model의 parameter값을 그대로 가져옴, False로 설정하면 모델의 아키텍처만 가져오고 parameter는 랜덤 설정\n",
        "num_ftrs = model.fc.in_features #모델의 마지막 레이어의 입력 채널의 수를 저장(in_features는 해당 레이어의 입력 채널 수를 의미)   \n",
        "model.fc = nn.Linear(num_ftrs, len(class_names)) #모델의 마지막 레이어를 새로운 레이어로 교체 (입력 채널 수는 기존 레이어와 동일, 출력 채널 수를 우리가 원하는 수로 설정하는 것! 여기서는 클래스 수 12개) \n",
        "\n",
        "'''\n",
        "#vgg16/19\n",
        "model = models.vgg16(pretrained=True)\n",
        "#model.classifier[6].out_features = len(class_names) #마지막 레이어를 교체하는 방법이 약간 다름, print 해서 구조 확인하면서 이해\n",
        "\n",
        "#mobilenet_v2\n",
        "model = models.mobilenet_v2(pretrained=True)\n",
        "#model.classifier[1].out_features = len(class_names)\n",
        "\n",
        "#mobilnet_v3_small\n",
        "model = models.mobilenet_v3_small(pretrained=True)\n",
        "#model.classifier[3].out_features = len(class_names)\n",
        "'''\n",
        "\n",
        "model = model.to(DEVICE) #모델 gpu에 태우기\n",
        "print(model)  #print(model)은 모델의 구조를 알 수 있음"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=2048, out_features=12, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Layer Freeze"
      ],
      "metadata": {
        "id": "4zzyFflRf13T"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wf8IIPgs3vs"
      },
      "source": [
        "cnt = 0 #몇 번째 Layer인지 나타내는 변수 cnt 설정\n",
        "for child in model.children(): #모델의 모든 Layer 정보를 담고 있음 (vgg, mobilenet 계열은 model.features)\n",
        "    cnt += 1 \n",
        "    #import pdb;pdb.set_trace()\n",
        "    if cnt < 8: #resnet50기준 10개의 Layer중 1~5개는 Freeze하고, 6~10은 학습 시 parameter를 업데이트 하도록!\n",
        "        #print(child)\n",
        "        for param in child.parameters(): #vgg, mobilenet 계열은 model.features.parameters()\n",
        "            param.requires_grad = False  #False -> NO UPDATE(FREEZE), True -> UPDATE(기본값)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. 손실함수, 최적화함수, 스케쥴러 설정\n",
        "- Adam vs SGD\n",
        "- learning rate는 작게!\n",
        "- 미리 학습 코드까지 실행!"
      ],
      "metadata": {
        "id": "onKCFqbZf9oZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 학습에 사용하는 Loss 함수를 지정\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#Optimizer는 Adam, filter와 lambda를 사용하는 이유 : param.requires_grad = True로 설정된 Layer의 parameter만을 업데이트 하기 위해서!\n",
        "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.00001) \n",
        " \n",
        "from torch.optim import lr_scheduler\n",
        "# 에포크에 따라 Learning Rate를 변경하는 역할 (7 에포크마다 0.1씩 곱해 LR을 감소시킴), Why? : 학습 보폭을 정하는 일은 매우 중요한데, 처음엔 크게 -> 학습 진행될 수록 작게 설정하는 것이 좋다고 알려짐, but 아직 연구중\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1) #7에포크마다 러닝레이트 0.1씩 곱해주겠다. 성능향상을 위해."
      ],
      "metadata": {
        "id": "LfwDUXcaD_uD"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. 모델 학습 및 저장"
      ],
      "metadata": {
        "id": "t86IqtKnK8Qr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXFjVMs3s5Jv"
      },
      "source": [
        "# 전이학습 모델 학습 및 검증\n",
        "import time\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n",
        "    \n",
        "    train_losses , train_accuracy = [],[] #그래프 그리기 위해서 train에 대한 loss,accuracy 저장 ##변수선언\n",
        "    val_losses , val_accuracy = [],[] #그래프 그리기 위해서 validation에 대한 loss,accuracy 저장\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())  #정확도가 가장 높은 모델을 저장\n",
        "    best_acc = 0.0 #정확도가 가장 높은 모델의 정확도 저장\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print('-------------- epoch {} ----------------'.format(epoch+1)) \n",
        "        since = time.time() #한 에포크 돌 때 소요되는 시간 측정(시작 시각 저장)                                    \n",
        "        for phase in ['train', 'val']: #한 에포크 돌 때 train 한 번, validation 한 번씩 각각 진행\n",
        "            if phase == 'train': \n",
        "                model.train() #train이면 학습 모드\n",
        "            else:\n",
        "                model.eval() #validation이면 평가 모드(평가 때 사용하지 말아야 할 작업들 알아서 꺼줌, dropout이나 batchnorm layer 같은 것들)     \n",
        " \n",
        "            running_loss = 0.0   #모든 데이터의 loss를 합해서 저장\n",
        "            running_corrects = 0 #정확하게 예측한 경우의 수를 저장\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]: #모델의 현재 모드(train or validation)에 해당하는 Dataloader에서 데이터를 받는 for문\n",
        "                inputs = inputs.to(DEVICE) #데이터를 gpu에 태움 \n",
        "                labels = labels.to(DEVICE) #데이터의 라벨값을 gpu에 태움\n",
        "                \n",
        "                optimizer.zero_grad() #학습 진행하면 이전 Batch의 Gradient값이 Optimizer에 저장될 것이므로 초기화 해주고 시작해야 함\n",
        "                \n",
        "                with torch.set_grad_enabled(phase == 'train'): #set_grad_enabled를 이용하면 train 모드에서만 모델의 Gradient를 업데이트 하도록 설정 할 수 있음\n",
        "                    outputs = model(inputs) #드디어 데이터를 모델에 입력!\n",
        "                    _, preds = torch.max(outputs, 1) #모델에 입력된 데이터가 12개의 클래스에 속할 확률값 출력, 이 중 가장 높은 값의 인덱스를 예측값으로 preds에 저장\n",
        "                    loss = criterion(outputs, labels) #모델의 예측값과 정답값 사이의 Loss를 계산(criterion 함수는 위에서 미리 설정해 둔 것)\n",
        "    \n",
        "                    if phase == 'train':   \n",
        "                        loss.backward() #계산한 loss값을 이용하여 BackPropagation을 통해 계산한 Gradient값을 parameter에 할당하고,\n",
        "                        optimizer.step() #모델의 parameter 업데이트\n",
        " \n",
        "                running_loss += loss.item() * inputs.size(0) #모든 데이터의 loss를 합해서 저장하기 위해, 하나의 미니 배치에 대한 loss값에 데이터의 수를 곱해서 더함 (inputs.size(0)이 미니 배치의 수) \n",
        "                running_corrects += torch.sum(preds == labels.data) #예측값과 정답값이 같으면 증가!\n",
        "\n",
        "            if phase == 'train':  \n",
        "                scheduler.step() #위에서 미리 설정한 Scheduler 실행\n",
        " \n",
        "            epoch_loss = running_loss/dataset_sizes[phase] #해당 에포크의 loss를 계산하기 위해 running_loss를 데이터셋 사이즈로 나눔\n",
        "            epoch_acc = running_corrects.double()/dataset_sizes[phase] #정확도도 마찬가지로 running_corrects를 데이터셋 사이즈로 나눔\n",
        " \n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc)) #해당 에포크의 loss와 정확도를 매번 출력\n",
        "\n",
        "            if phase == 'train': #그래프 그리기 위해 train 데이터의 loss와 accuracy 따로 저장\n",
        "                train_losses.append(epoch_loss)\n",
        "                train_accuracy.append(epoch_acc)\n",
        "            if phase == 'val': #그래프 그리기 위해 \bvalidation 데이터의 loss와 accuracy 따로 저장\n",
        "                val_losses.append(epoch_loss)\n",
        "                val_accuracy.append(epoch_acc)\n",
        "          \n",
        "            if phase == 'val' and epoch_acc > best_acc: #validation 모드에서 정확도가 최고 정확도 보다 높으면 업데이트\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict()) #최고 정확도를 가진 모델을 best_model_wts 변수에 저장\n",
        " \n",
        "        time_elapsed = time.time() - since #한 에포크 돌 때 소요되는 시간 측정(종료 시각 - 시작 시각) \n",
        "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) #계산한 시간 분과 초로 출력\n",
        "\n",
        "    #학습 종료 후 \n",
        "    print('Best validation Acc: {:4f}'.format(best_acc)) #validation 중 최고 정확도 출력\n",
        "\n",
        "    #train과 validation의 loss, accuracy 그래프 출력 -> 과적합 여부 등 판단\n",
        "    plt.plot(range(1,len(train_losses)+1),train_losses,'bo',label = 'training loss')\n",
        "    plt.plot(range(1,len(val_losses)+1),val_losses,'r',label = 'validation loss')\n",
        "    plt.legend()\n",
        "    plt.plot(range(1,len(train_accuracy)+1),train_accuracy,'co',label = 'training accuracy')\n",
        "    plt.plot(range(1,len(val_accuracy)+1),val_accuracy,'m',label = 'validation accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    #정확도가 가장 높았던 모델을 불러와서 반환\n",
        "    model.load_state_dict(best_model_wts) \n",
        "    return model"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1069
        },
        "id": "EQ6wBtMAs6pw",
        "outputId": "17cd2565-a46b-45af-ab31-11abdda61679"
      },
      "source": [
        "# 전이학습 실행\n",
        "model = train_model(model, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=EPOCH) \n",
        "\n",
        "# 반환 받은 정확도가 가장 높았던 모델을 torch.save 이용해서 저장 (모델 별로 이름 변경해서 저장!)\n",
        "torch.save(model, '/content/drive/MyDrive/plant-leaf-dataset/resnet50.pt')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------- epoch 1 ----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 2.4152 Acc: 0.1641\n",
            "val Loss: 2.1574 Acc: 0.3517\n",
            "Completed in 0m 23s\n",
            "-------------- epoch 2 ----------------\n",
            "train Loss: 2.0041 Acc: 0.4711\n",
            "val Loss: 1.7726 Acc: 0.6177\n",
            "Completed in 0m 23s\n",
            "-------------- epoch 3 ----------------\n",
            "train Loss: 1.6419 Acc: 0.6765\n",
            "val Loss: 1.4623 Acc: 0.7216\n",
            "Completed in 0m 23s\n",
            "-------------- epoch 4 ----------------\n",
            "train Loss: 1.3256 Acc: 0.7550\n",
            "val Loss: 1.1778 Acc: 0.7793\n",
            "Completed in 0m 22s\n",
            "-------------- epoch 5 ----------------\n",
            "train Loss: 1.0783 Acc: 0.7960\n",
            "val Loss: 0.9545 Acc: 0.8259\n",
            "Completed in 0m 23s\n",
            "-------------- epoch 6 ----------------\n",
            "train Loss: 0.8789 Acc: 0.8262\n",
            "val Loss: 0.7862 Acc: 0.8427\n",
            "Completed in 0m 22s\n",
            "-------------- epoch 7 ----------------\n",
            "train Loss: 0.7149 Acc: 0.8507\n",
            "val Loss: 0.6409 Acc: 0.8616\n",
            "Completed in 0m 23s\n",
            "-------------- epoch 8 ----------------\n",
            "train Loss: 0.6400 Acc: 0.8599\n",
            "val Loss: 0.6287 Acc: 0.8659\n",
            "Completed in 0m 23s\n",
            "-------------- epoch 9 ----------------\n",
            "train Loss: 0.6365 Acc: 0.8570\n",
            "val Loss: 0.6057 Acc: 0.8698\n",
            "Completed in 0m 23s\n",
            "-------------- epoch 10 ----------------\n",
            "train Loss: 0.6208 Acc: 0.8608\n",
            "val Loss: 0.6052 Acc: 0.8685\n",
            "Completed in 0m 22s\n",
            "Best validation Acc: 0.869828\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjkyMbUEuMqi"
      },
      "source": [
        "### 7. 모델 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "fPwYDjEHtCXm",
        "outputId": "67b4c2bf-378a-4816-df45-3da65442d5dc"
      },
      "source": [
        "# 전이학습 평가 전처리 (위에서 설명한 것과 동일)\n",
        "data_transforms = transforms.Compose([ \n",
        "        transforms.Resize([64,64]),  \n",
        "        transforms.RandomCrop(52),  \n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
        "        ])\n",
        "\n",
        "#경로 맞춰서 변경해 주세요!\n",
        "test_dataset = ImageFolder(root='/content/drive/MyDrive/plant-leaf-dataset/plant-leaf-new-dataset/test', transform=data_transforms) \n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "# 모델 평가 함수\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval() #모델을 평가 모드로 설정\n",
        "    test_loss = 0 #미니 배치 별로 loss를 합산해서 저장\n",
        "    correct = 0 #정확하게 예측한 수 저장   \n",
        "    with torch.no_grad(): #해당 메서드를 이용해서 parameter 업데이트 방지\n",
        "        for data, target in test_loader:  \n",
        "            data, target = data.to(DEVICE), target.to(DEVICE) #데이터와 라벨을 불러오면서 gpu에 태움  \n",
        "            output = model(data) #데이터를 모델에 입력           \n",
        "            test_loss += torch.nn.functional.cross_entropy(output,target, reduction='sum').item() #모델의 예측값과 정답값 사이의 loss 계산\n",
        "            pred = output.max(1, keepdim=True)[1]  #모델에 입력된 데이터가 12개의 클래스에 속할 확률값 출력, 이 중 가장 높은 값의 인덱스를 예측값으로 pred에 저장\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item() #target.view_as(pred)를 이용해 target의 텐서 구조를 pred의 텐서와 같은 모양으로 재정렬 (모델 만들 때 쓰는 view와 비슷 view는 숫자 직접 지정)\n",
        "                                                                  #eq는 비교 연산자로 pred와 target.view_as(pred)의 값이 일치하면 1, 일치하지 않으면 0 반환\n",
        "   \n",
        "    test_loss /= len(test_loader.dataset) #모든 미니 배치에서 합한 loss값을 배치 수로 나누어 loss값의 평균 구함\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset) #마찬가지로 정확도의 평균도 구함\n",
        "    \n",
        "    return test_loss, test_accuracy #계산한 Test 데이터의 loss와 정확도 반환\n",
        "\n",
        "# 전이학습 모델 평가 결과\n",
        "model=torch.load('/content/drive/MyDrive/plant-leaf-dataset/resnet50.pt') #torch.load를 이용해서 원하는 모델 불러오기!\n",
        "test_loss, test_accuracy = evaluate(model, test_loader) #평가 함수 이용해서 Test 데이터에 대한 loss 및 정확도 측정\n",
        "print('model test acc:  ', test_accuracy) #평가 정확도 출력"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model test acc:   85.81896551724138\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(과제) 한 가지 이상의 변화를 준 후 학습을 돌려서 결과와 함께 간단한 설명을 업로드 해주세요 😀\n",
        "\n",
        "예시 : 다른 전이학습 모델 사용, freeze 시키는 구간 변화, 직접 짠 모델과의 성능 비교, 데이터 수의 변화, optimizer에 대한 실험, epoch 늘리기, 등등"
      ],
      "metadata": {
        "id": "maEk9ITatoai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "에포크=10, freeze layer:7"
      ],
      "metadata": {
        "id": "ysuaJi6xEnzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "spKg7YBdHPjr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}